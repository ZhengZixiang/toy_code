{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 个性化推荐\n",
    "本项目使用文本卷积神经网络，并使用[`MovieLens`](https://grouplens.org/datasets/movielens/)数据集完成电影推荐的任务。\n",
    "\n",
    "推荐系统在日常的网络应用中无处不在，比如网上购物，网上买书、新闻app、社交网络、音乐网站、电影网站等等等等，有人的地方就有推荐。根据个人的喜好，相同喜好人群的习惯等信息进行个性化的内容推荐。比如打开新闻类的app，因为有了个性化的内容，每个人看到的新闻首页都是不一样的。\n",
    "\n",
    "这当然是很有用的，在信息爆炸的今天，获取信息的途径和方式多种多样，人们花费时间最多的不再是去哪获取信息，而是要在众多的信息中寻找自己感兴趣的，这就是信息超载问题。为了解决这个问题，推荐系统应运而生。\n",
    "\n",
    "协同过滤是推荐系统应用较为广泛的技术，该方法搜集用户的历史纪录、个人喜好等信息，计算与其他用户的相似度，利用相似用户的评价来预测目标用户对特定项目的喜好程度。有点是会给用户推荐未浏览过的项目，缺点呢，对于新用户来说，没有任何与商品的交互记录和个人喜好等信息，存在冷启动问题，导致模型无法找到相似的用户或商品。\n",
    "\n",
    "为了解决冷启动的问题，通常的做法是对于刚注册的用户，要求用户先选择自己感兴趣的话题、群组、商品、性格、喜欢的音乐类型的信息，比如豆瓣FM。\n",
    "\n",
    "## 下载数据集\n",
    "运行下面代码把[数据集](http://files.grouplens.org/datasets/movielens/ml-1m.zip)下载下来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.ops import math_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import zipfile\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "\n",
    "def _unzip(save_path, _, database_name, data_path):\n",
    "    \"\"\"\n",
    "    Unzip wrapper with the same interface as _ungzip\n",
    "    :param save_path: The path of the gzip files\n",
    "    :param database_name: Name of database\n",
    "    :param data_path: Path to extract to\n",
    "    :param _: HACK - Used to have to same interface as _ungzip\n",
    "    \"\"\"\n",
    "    print('Extracting {} ...'.format(database_name))\n",
    "    with zipfile.ZipFile(save_path) as zf:\n",
    "        zf.extractall(data_path)\n",
    "        \n",
    "\n",
    "def download_extract(database_name, data_path):\n",
    "    \"\"\"\n",
    "    Download and extract database\n",
    "    :param database_name: Database name\n",
    "    \"\"\"\n",
    "    DATASET_ML1M = 'ml-1m'\n",
    "    \n",
    "    if database_name == DATASET_ML1M:\n",
    "        url = 'http://files.grouplens.org/datasets/movielens/ml-1m.zip'\n",
    "        hash_code = 'c4d9eecfca2ab87c1945afe126590906'\n",
    "        extract_path = os.path.join(data_path, 'ml-1m')\n",
    "        save_path = os.path.join(data_path, 'ml-1m.zip')\n",
    "        extract_fn = _unzip\n",
    "    \n",
    "    if os.path.exists(extract_path):\n",
    "        print('Found {} Data'.format(database_name))\n",
    "        return\n",
    "    \n",
    "    if not os.path.exists(data_path):\n",
    "        os.makedirs(data_path)\n",
    "    \n",
    "    if not os.path.exists(save_path):\n",
    "        with DLProgress(unit='B', unit_scale=True, miniters=1, desc='Downloading {}'.format(database_name)) as pbar:\n",
    "            urlretrieve(url, save_path, pbar.hook)\n",
    "    \n",
    "    assert hashlib.md5(open(save_path, 'rb').read()).hexdigest() == hash_code, \\\n",
    "        '{} file is corrupted. Remove the file and try again.'.format(save_path)\n",
    "    \n",
    "    os.makedirs(extract_path)\n",
    "    try:\n",
    "        extract_fn(save_path, extract_path, database_name, data_path)\n",
    "    except Exception as err:\n",
    "        shutil.rmtree(extract_path)  # Remove extraction folder if there is an error\n",
    "        raise err\n",
    "    \n",
    "    print('Done.')\n",
    "    \n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    \"\"\"\n",
    "    Handle Progress Bar while Downloading\n",
    "    \"\"\"\n",
    "    last_block = 0\n",
    "    \n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        \"\"\"\n",
    "        A hook function that will be called once on establishment of the network connection\n",
    "        and once after each block read there after\n",
    "        :param block_num: A count of blocks transferred so far\n",
    "        :param block_size: Block size in bytes\n",
    "        :param total_size: The total size of the file. This may be -1 on older FTP servers\n",
    "                           which do not return a file size in response to a retrieval request.\n",
    "        \"\"\"\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found ml-1m Data\n"
     ]
    }
   ],
   "source": [
    "data_dir = './'\n",
    "download_extract('ml-1m', data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 先来看看数据\n",
    "本项目使用的是MovieLens 1M 数据集，包含6000个用户在近4000部电影上的1亿条评论。\n",
    "数据集分为三个文件：用户数据users.dat，电影数据movies.dat和评分数据ratings.dat。\n",
    "\n",
    "### 用户数据\n",
    "分别有用户ID、性别、年龄、职业ID和邮编等字段。\n",
    "\n",
    "数据中的格式：UserID::Gender::Age::Occupation::Zip-code\n",
    "- Gender is denoted by a \"M\" for male and \"F\" for female\n",
    "- Age is chosen from the following ranges:\n",
    "\t*  1:  \"Under 18\"\n",
    "\t* 18:  \"18-24\"\n",
    "\t* 25:  \"25-34\"\n",
    "\t* 35:  \"35-44\"\n",
    "\t* 45:  \"45-49\"\n",
    "\t* 50:  \"50-55\"\n",
    "\t* 56:  \"56+\"\n",
    "- Occupation is chosen from the following choices:\n",
    "\t*  0:  \"other\" or not specified\n",
    "\t*  1:  \"academic/educator\"\n",
    "\t*  2:  \"artist\"\n",
    "\t*  3:  \"clerical/admin\"\n",
    "\t*  4:  \"college/grad student\"\n",
    "\t*  5:  \"customer service\"\n",
    "\t*  6:  \"doctor/health care\"\n",
    "\t*  7:  \"executive/managerial\"\n",
    "\t*  8:  \"farmer\"\n",
    "\t*  9:  \"homemaker\"\n",
    "\t* 10:  \"K-12 student\"\n",
    "\t* 11:  \"lawyer\"\n",
    "\t* 12:  \"programmer\"\n",
    "\t* 13:  \"retired\"\n",
    "\t* 14:  \"sales/marketing\"\n",
    "\t* 15:  \"scientist\"\n",
    "\t* 16:  \"self-employed\"\n",
    "\t* 17:  \"technician/engineer\"\n",
    "\t* 18:  \"tradesman/craftsman\"\n",
    "\t* 19:  \"unemployed\"\n",
    "\t* 20:  \"writer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>OccupationID</th>\n",
       "      <th>Zip-code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>56</td>\n",
       "      <td>16</td>\n",
       "      <td>70072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "      <td>55117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>M</td>\n",
       "      <td>45</td>\n",
       "      <td>7</td>\n",
       "      <td>02460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>20</td>\n",
       "      <td>55455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID Gender  Age  OccupationID Zip-code\n",
       "0       1      F    1            10    48067\n",
       "1       2      M   56            16    70072\n",
       "2       3      M   25            15    55117\n",
       "3       4      M   45             7    02460\n",
       "4       5      M   25            20    55455"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_title = ['UserID', 'Gender', 'Age', 'OccupationID', 'Zip-code']\n",
    "users = pd.read_table('./ml-1m/users.dat', sep='::', header=None, names=users_title, engine='python')\n",
    "users.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出UserID、Gender、Age和Occupation都是类别字段，其中邮编字段是我们不使用的。\n",
    "\n",
    "### 电影数据\n",
    "分别有电影ID、电影名和电影风格等字段。\n",
    "\n",
    "数据中的格式：MovieID::Title::Genres\n",
    "- Titles are identical to titles provided by the IMDB (including\n",
    "year of release)\n",
    "- Genres are pipe-separated and are selected from the following genres:\n",
    "\t* Action\n",
    "\t* Adventure\n",
    "\t* Animation\n",
    "\t* Children's\n",
    "\t* Comedy\n",
    "\t* Crime\n",
    "\t* Documentary\n",
    "\t* Drama\n",
    "\t* Fantasy\n",
    "\t* Film-Noir\n",
    "\t* Horror\n",
    "\t* Musical\n",
    "\t* Mystery\n",
    "\t* Romance\n",
    "\t* Sci-Fi\n",
    "\t* Thriller\n",
    "\t* War\n",
    "\t* Western"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Animation|Children's|Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children's|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MovieID                               Title                        Genres\n",
       "0        1                    Toy Story (1995)   Animation|Children's|Comedy\n",
       "1        2                      Jumanji (1995)  Adventure|Children's|Fantasy\n",
       "2        3             Grumpier Old Men (1995)                Comedy|Romance\n",
       "3        4            Waiting to Exhale (1995)                  Comedy|Drama\n",
       "4        5  Father of the Bride Part II (1995)                        Comedy"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_title = ['MovieID', 'Title', 'Genres']\n",
    "movies = pd.read_table('./ml-1m/movies.dat', sep='::', header=None, names=movies_title, engine='python')\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MovieID是类别字段，Title是文本，Genres也是类别字段\n",
    "\n",
    "### 评分数据\n",
    "分别有用户ID、电影ID、评分和时间戳等字段。\n",
    "\n",
    "数据中的格式：UserID::MovieID::Rating::Timestamp\n",
    "- UserIDs range between 1 and 6040 \n",
    "- MovieIDs range between 1 and 3952\n",
    "- Ratings are made on a 5-star scale (whole-star ratings only)\n",
    "- Timestamp is represented in seconds since the epoch as returned by time(2)\n",
    "- Each user has at least 20 ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>978300275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  MovieID  Rating  timestamp\n",
       "0       1     1193       5  978300760\n",
       "1       1      661       3  978302109\n",
       "2       1      914       3  978301968\n",
       "3       1     3408       4  978300275\n",
       "4       1     2355       5  978824291"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_title = ['UserID', 'MovieID', 'Rating', 'timestamp']\n",
    "ratings = pd.read_table('./ml-1m/ratings.dat', sep='::', header=None, names=ratings_title, engine='python')\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "评分字段Rating就是我们要学习的targets，时间戳字段我们不使用。\n",
    "\n",
    "## 来说说数据预处理\n",
    "- UserID、Occupation和MovieID不用变\n",
    "- Gender字段：需要将“F”和“M”转换成0和1\n",
    "- Age字段：要转成7个连续数字0~6\n",
    "- Genres字段：是分类字段，要转成数字，首先将Genres中的类别转成字符串到数字的字典，然后将每个电影的Genres字段转成数字列表，因为有些电影是多个Genres的组合。\n",
    "- Title字段：处理方式跟Genres字段一样，首先创建文本到数字的字典，然后将Title中的描述转成数字的列表。另外Title中的年份也需要去掉。\n",
    "- Genres和Title字段需要将长度统一，这样在神经网络中方便处理。空白部分用< PAD >对应的数字填充\n",
    "\n",
    "## 实现数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load Dataset from File\n",
    "    \"\"\"\n",
    "    # 读取User数据\n",
    "    users_title = ['UserID', 'Gender', 'Age', 'JobID', 'Zip-code']\n",
    "    users = pd.read_table('./ml-1m/users.dat', sep='::', header=None, names=users_title, engine='python')\n",
    "    users = users.filter(regex='UserID|Gender|Age|JobID')\n",
    "    users_orig = users.values\n",
    "    # 改版User数据中性别和奈年龄\n",
    "    gender_map = {'F': 0, 'M': 1}\n",
    "    users['Gender'] = users['Gender'].map(gender_map)\n",
    "    age_map = {val: ii for ii, val in enumerate(set(users['Age']))}\n",
    "    users['Age'] = users['Age'].map(age_map)\n",
    "    \n",
    "    # 读取Movie数据集\n",
    "    movies_title=['MovieID', 'Title', 'Genres']\n",
    "    movies = pd.read_table('./ml-1m/movies.dat', sep='::', header=None, names=movies_title, engine='python')\n",
    "    movies_orig = movies.values\n",
    "    # 将Title中的年份去掉\n",
    "    pattern = re.compile(r'^(.*)\\((\\d+)\\)$')\n",
    "    title_map = {val: pattern.match(val).group(1) for ii, val in enumerate(set(movies['Title']))}\n",
    "    movies['Title'] = movies['Title'].map(title_map)\n",
    "    \n",
    "    # 电影类型转数字字典\n",
    "    genres_set = set()\n",
    "    for val in movies['Genres'].str.split('|'):\n",
    "        genres_set.update(val)\n",
    "    \n",
    "    genres_set.add('<PAD>')\n",
    "    genres2int = {val: ii for ii, val in enumerate(genres_set)}\n",
    "    \n",
    "    # 将电影类型转成等长数字列表，长度是18\n",
    "    genres_map = {val: [genres2int[row] for row in val.split('|')] for ii, val in enumerate(set(movies['Genres']))}\n",
    "    for key in genres_map:\n",
    "        for cnt in range(max(genres2int.values()) - len(genres_map[key])):\n",
    "            genres_map[key].insert(len(genres_map[key]) + cnt, genres2int['<PAD>'])\n",
    "    \n",
    "    movies['Genres'] = movies['Genres'].map(genres_map)\n",
    "    \n",
    "    # 电影Title转数字字典\n",
    "    title_set = set()\n",
    "    for val in movies['Title'].str.split():\n",
    "        title_set.update(val)\n",
    "    \n",
    "    title_set.add('<PAD>')\n",
    "    title2int = {val: ii for ii, val in enumerate(title_set)}\n",
    "    \n",
    "    # 将电影Title转成等长数字列表，长度是15\n",
    "    title_count = 15\n",
    "    title_map = {val: [title2int[row] for row in val.split()] for ii, val in enumerate(set(movies['Title']))}\n",
    "    \n",
    "    for key in title_map:\n",
    "        for cnt in range(title_count - len(title_map[key])):\n",
    "            title_map[key].insert(len(title_map[key]) + cnt, title2int['<PAD>'])\n",
    "    \n",
    "    movies['Title'] = movies['Title'].map(title_map)\n",
    "    \n",
    "    # 读取评分数据集\n",
    "    ratings_title = ['UserID', 'MovieID', 'ratings', 'timestamps']\n",
    "    ratings = pd.read_table('./ml-1m/ratings.dat', sep='::', header=None, names=ratings_title, engine='python')\n",
    "    ratings = ratings.filter(regex='UserID|MovieID|ratings')\n",
    "    \n",
    "    # 合并三个表\n",
    "    data = pd.merge(pd.merge(ratings, users), movies)\n",
    "    \n",
    "    # 将数据分成X和y两张表\n",
    "    target_fields = ['ratings']\n",
    "    features_pd, targets_pd = data.drop(target_fields, axis=1), data[target_fields]\n",
    "    \n",
    "    features = features_pd.values\n",
    "    targets_values = targets_pd.values\n",
    "    \n",
    "    return title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载数据并保存到本地\n",
    "- title_count：Title字段的长度（15）\n",
    "- title_set：Title文本的集合\n",
    "- genres2int：电影类型转数字的字典\n",
    "- features：是输入X\n",
    "- targets_values：是学习目标y\n",
    "- ratings：评分数据集的Pandas对象\n",
    "- users：用户数据集的Pandas对象\n",
    "- movies：电影数据的Pandas对象\n",
    "- data：三个数据集组合在一起的Pandas对象\n",
    "- movies_orig：没有做数据处理的原始电影数据\n",
    "- users_orig：没有做数据处理的原始用户数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig = load_data()\n",
    "\n",
    "pickle.dump((title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig), open('preprocess.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预处理后的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>JobID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  Gender  Age  JobID\n",
       "0       1       0    0     10\n",
       "1       2       1    5     16\n",
       "2       3       1    6     15\n",
       "3       4       1    2      7\n",
       "4       5       1    6     20"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[2511, 1642, 2846, 2846, 2846, 2846, 2846, 284...</td>\n",
       "      <td>[4, 5, 18, 12, 12, 12, 12, 12, 12, 12, 12, 12,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[1084, 2846, 2846, 2846, 2846, 2846, 2846, 284...</td>\n",
       "      <td>[6, 5, 9, 12, 12, 12, 12, 12, 12, 12, 12, 12, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[3687, 4098, 1177, 2846, 2846, 2846, 2846, 284...</td>\n",
       "      <td>[18, 16, 12, 12, 12, 12, 12, 12, 12, 12, 12, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[3199, 1612, 3430, 2846, 2846, 2846, 2846, 284...</td>\n",
       "      <td>[18, 1, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[3044, 5060, 4545, 3922, 257, 2540, 2846, 2846...</td>\n",
       "      <td>[18, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MovieID                                              Title  \\\n",
       "0        1  [2511, 1642, 2846, 2846, 2846, 2846, 2846, 284...   \n",
       "1        2  [1084, 2846, 2846, 2846, 2846, 2846, 2846, 284...   \n",
       "2        3  [3687, 4098, 1177, 2846, 2846, 2846, 2846, 284...   \n",
       "3        4  [3199, 1612, 3430, 2846, 2846, 2846, 2846, 284...   \n",
       "4        5  [3044, 5060, 4545, 3922, 257, 2540, 2846, 2846...   \n",
       "\n",
       "                                              Genres  \n",
       "0  [4, 5, 18, 12, 12, 12, 12, 12, 12, 12, 12, 12,...  \n",
       "1  [6, 5, 9, 12, 12, 12, 12, 12, 12, 12, 12, 12, ...  \n",
       "2  [18, 16, 12, 12, 12, 12, 12, 12, 12, 12, 12, 1...  \n",
       "3  [18, 1, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12...  \n",
       "4  [18, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 1...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1,\n",
       "       list([2511, 1642, 2846, 2846, 2846, 2846, 2846, 2846, 2846, 2846, 2846, 2846, 2846, 2846, 2846]),\n",
       "       list([4, 5, 18, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从本地读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig = pickle.load(open('preprocess.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型设计\n",
    "通过研究数据集中的字段类型，我们发现有一些是类别字段，通常的处理是将这些字段转成one hot编码，但是像UserID、MovieID这样的字段就会变成非常的稀疏，输入的维度急剧膨胀，这是我们不愿意见到的，毕竟我们的电脑不像大厂动辄能处理数以亿计维度的输入:)\n",
    "\n",
    "所以在与处理数据时将这些字段转成了数字，我们用这个数字当作嵌入矩阵的索引，在网络的第一层使用了嵌入层，维度是(N，32)和(N，16)\n",
    "\n",
    "电影类型的处理要多一步，有时一个电影有多个电影类型，这样从嵌入矩阵索引出来是一个(n, 32)的矩阵，因为有多个类型嘛，我们要将这个矩阵求和，变成(1,32)的向量。\n",
    "\n",
    "电影名的处理比较特殊，没有使用循环神经网络，而是用了文本卷积网络，下文会进行说明。\n",
    "\n",
    "从嵌入层索引出特征以后，将各特征传入全连接层，将输出再次传入全连接层，最终分别得到(1, 200)的用户特征和电影特征两个特征向量。\n",
    "\n",
    "我们的目的就是要训练出用户特征和电影特征，在实现推荐功能时使用。得到这两个特征以后，就可以选择任意的方式来拟合评分了。我使用了两种方式，一个是将两个特征向量做乘法，将结果与真实评分做回归，采用MSE优化损失。因为本质上这是一个回归问题，另一种方式是，将两个特征作为输入，再次传入全连接层，输出一个值，将输出值回归到真实评分，采用MSE优化损失。\n",
    "\n",
    "实际上第二个方式的MSE loss在0.8附近，第一个方式在1附近，5次迭代的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本卷积网络\n",
    "网络的第一层是词嵌入层，由每一个单词的嵌入向量组成的嵌入矩阵。下一层使用多个不同尺寸的卷积核在嵌入矩阵上做卷积，窗口大小指的是每次卷积覆盖几个单词。这里跟对图像做卷积不太一样，图像的卷积通常用2x2、3x3、5x5之类的尺寸，而文本卷积要覆盖整个单词的嵌入向量，所以尺寸是（单词数、向量维度），比如每次滑动3个、4个或者5个单词。第三层网络是max pooling得到一个常相良，最后使用dropout做正则化，最终得到了电影Title的特征。\n",
    "\n",
    "## 辅助函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def save_params(params):\n",
    "    \"\"\"\n",
    "    Save parameters to file\n",
    "    \"\"\"\n",
    "    pickle.dump(params, open('params.p', 'wb'))\n",
    "    \n",
    "\n",
    "def load_params():\n",
    "    \"\"\"\n",
    "    Load parameters from file\n",
    "    \"\"\"\n",
    "    return pickle.load(open('params.p', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 嵌入矩阵的维度\n",
    "embed_dim = 32\n",
    "# 用户ID个数\n",
    "uid_max = max(features.take(0, 1)) + 1  # 6040\n",
    "# 性别个数\n",
    "gender_max = max(features.take(2, 1)) + 1  # 2\n",
    "# 年龄类别个数\n",
    "age_max = max(features.take(3, 1)) + 1  # 7\n",
    "# 职业个数\n",
    "job_max = max(features.take(4, 1)) + 1  # 21\n",
    "\n",
    "# 电影ID个数\n",
    "movie_id_max = max(features.take(1, 1)) + 1  #3952\n",
    "# 电影类型个数\n",
    "movie_categories_max = max(genres2int.values()) + 1  # 19\n",
    "# 电影名单词个数\n",
    "movie_title_max = len(title_set)  # 5216\n",
    "\n",
    "# 对电影类型嵌入向量做加和操作的标志，考虑过使用mean做平均，但是没实现mean\n",
    "combiner = 'sum'\n",
    "\n",
    "# 电影名长度\n",
    "sentences_size = title_count  # 15\n",
    "# 文本卷积滑动窗口，分别华东2，3，4，5个单词\n",
    "# window_sizes = [2, 3, 4, 5]\n",
    "# 文本卷积核数量\n",
    "filter_num = 8\n",
    "\n",
    "# 电影ID转下标的字典，数据集中电影ID跟下标不一致，比如第5行的数据电影ID不一定是5\n",
    "movieid2idx = {val[0]:i for i, val in enumerate(movies.values)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 超参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "batch_size = 256\n",
    "dropout_keep = 0.5\n",
    "learning_rate = 0.0001\n",
    "show_every_n_batches = 20\n",
    "save_dir = './save'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输入\n",
    "定义输入的占位符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs():\n",
    "    uid = tf.placeholder(tf.int32, [None, 1], name='uid')\n",
    "    movie_id = tf.placeholder(tf.int32, [None, 1], name='movie_id')\n",
    "    movie_categories = tf.placeholder(tf.int32, [None, 18], name='movie_categories')\n",
    "    movie_titles = tf.placeholder(tf.int32, [None, 15], name='movie_titles')\n",
    "    targets = tf.placeholder(tf.int32, [None, 1], name='targets')\n",
    "    LearningRate = tf.placeholder(tf.float32, name='LearningRate')\n",
    "    dropout_keep_prob = tf.placeholder(tf.float32, name='dropout_keep_prob')\n",
    "    return uid, movie_id, movie_categories, movie_titles, targets, LearningRate, dropout_keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建神经网络\n",
    "#### 定义User的嵌入矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uid_embed_layer(uid):\n",
    "    with tf.name_scope('user_embedding'):\n",
    "        uid_embed_matrix = tf.Variable(tf.random_uniform([uid_max, embed_dim], -1, 1), name='uid_embed_matrix')\n",
    "        uid_embed_layer = tf.nn.embedding_lookup(uid_embed_matrix, uid, name='uid_embed_layer')\n",
    "    return uid_embed_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 将User的嵌入矩阵一起全连接生成User的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_feature_layer(uid_embed_layer):\n",
    "    with tf.name_scope('user_fc'):\n",
    "        # 第一层全连接\n",
    "        uid_fc_layer = tf.layers.dense(uid_embed_layer, embed_dim, name='uid_fc_layer')\n",
    "        \n",
    "        # 第二层全连接\n",
    "        user_combine_layer = tf.contrib.layers.fully_connected(uid_fc_layer, 200, tf.tanh)\n",
    "        \n",
    "        user_combine_layer_flat = tf.reshape(user_combine_layer, [-1, 200])\n",
    "        \n",
    "    return user_combine_layer, user_combine_layer_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义Movie ID的嵌入矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_id_embed_layer(movie_id):\n",
    "    with tf.name_scope('movie_embedding'):\n",
    "        movie_id_embed_matrix = tf.Variable(tf.random_uniform([movie_id_max, embed_dim], -1, 1), name='movie_id_embed_matrix')\n",
    "        movie_id_embed_layer = tf.nn.embedding_lookup(movie_id_embed_matrix, movie_id, name='movie_id_embed_layer')\n",
    "    return movie_id_embed_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 对电影类型的多个嵌入向量做加和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_categories_layers(movie_categories):\n",
    "    with tf.name_scope('movie_categories_layers'):\n",
    "        movie_categories_embed_matrix = tf.Variable(tf.random_uniform([movie_categories_max, embed_dim], -1, 1), name='movie_categories_embed_matrix')\n",
    "        movie_categories_embed_layer = tf.nn.embedding_lookup(movie_categories_embed_matrix, movie_categories, name='movie_categories_embed_layer')\n",
    "        if combiner == 'sum':\n",
    "            movie_categories_embed_layer = tf.reduce_sum(movie_categories_embed_layer, axis=1, keep_dims=True)\n",
    "    return movie_categories_embed_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Movie Title的文本卷积网络实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_cnn_layer(movie_titles):\n",
    "    # 从嵌入矩阵中得到电影名对应的各个单词的嵌入向量\n",
    "    with tf.name_scope('movie_embedding'):\n",
    "        movie_title_embed_matrix = tf.Variable(tf.random_uniform([movie_title_max, embed_dim], -1, 1), name='movie_title_embed_matrix')\n",
    "        movie_title_embed_layer = tf.nn.embedding_lookup(movie_title_embed_matrix, movie_titles, name='movie_title_embed_layer')\n",
    "        movie_title_embed_layer_expand = tf.expand_dims(movie_title_embed_layer, -1)\n",
    "    \n",
    "    # 对文本嵌入层使用不同尺寸的卷积核做卷积和最大池化\n",
    "    pool_layer_lst = []\n",
    "    for window_size in window_sizes:\n",
    "        with tf.name_scope('movie_txt_conv_maxpool_{}'.format(window_size)):\n",
    "            filter_weights = tf.Variable(tf.truncated_normal([window_size, embed_dim, 1, filter_num], stddev=0.1), name='filter_weights')\n",
    "            filter_bias = tf.Variable(tf.constant(0.1, shape=[filter_num]), name='filter_bias')\n",
    "            \n",
    "            conv_layer = tf.nn.conv2d(movie_title_embed_layer_expand, filter_weights, [1, 1, 1, 1], padding='VALID', name='conv_layer')\n",
    "            relu_layer = tf.nn.relu(tf.nn.bias_add(conv_layer, filter_bias), name='relu_layer')\n",
    "            maxpool_layer = tf.nn.max_pool(relu_layer, [1, sentences_size-window_size+1, 1, 1], [1, 1, 1, 1], padding='VALID', name='maxpool_layer')\n",
    "            pool_layer_lst.append(maxpool_layer)\n",
    "        \n",
    "    # Dropout层\n",
    "    with tf.name_scope('pool_dropout'):\n",
    "        pool_layer = tf.concate(pool_layer_lst, 3, name='pool_layer')\n",
    "        max_num = len(window_sizes) * filter_num\n",
    "        pool_layer_flat = tf.reshape(pool_layer, [-1, 1, max_num], name = 'pool_layer_flat')\n",
    "        dropout_layer = tf.nn.dropout(pool_layer_flat, dropout_keep_prob, name='dropout_layer')\n",
    "    return pool_layer_flat, dropout_layer\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 将Movie的各个层一起做全连接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_feature_layer(movie_id_embed_layer):\n",
    "    with tf.name_scope('movie_fc'):\n",
    "        # 第一层全连接\n",
    "        movie_id_fc_layer = tf.layers.dense(movie_id_embed_layer, embed_dim, name='movie_id_fc_layer', activation=tf.nn.relu)\n",
    "        # 第二层全连接\n",
    "        movie_combine_layer = tf.contrib.layers.fully_connected(movie_id_fc_layer, 200, tf.tanh)\n",
    "        movie_combine_layer_flat = tf.reshape(movie_combine_layer, [-1, 200])\n",
    "    return movie_combine_layer, movie_combine_layer_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建计算图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    # 获取输入占位符\n",
    "    uid, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob = get_inputs()\n",
    "    # 获取User的4个嵌入想浪\n",
    "    uid_embed_layer = get_uid_embed_layer(uid)\n",
    "    # 得到用户特征\n",
    "    user_combine_layer, user_combine_layer_flat = get_user_feature_layer(uid_embed_layer)\n",
    "    # 获取电影ID的嵌入向量\n",
    "    movie_id_embed_layer = get_movie_id_embed_layer(movie_id)\n",
    "    # 得到电影特征\n",
    "    movie_combine_layer, movie_combine_layer_flat = get_movie_feature_layer(movie_id_embed_layer)\n",
    "    # 计算出评分，要注意两个不同的方案，inference的名字（name值）是不一样的，后面做推荐时要根据name取得tensor\n",
    "    with tf.name_scope('inference'):\n",
    "        # 将用户特征和电影特征作为输入，经过全连接，输出一个值得方案\n",
    "        # 简单地将用户特征和电影特征做矩阵乘法得到一个预测评分\n",
    "        inference = tf.reduce_sum(user_combine_layer_flat * movie_combine_layer_flat, axis=1)\n",
    "        inference = tf.expand_dims(inference, axis=1)\n",
    "    with tf.name_scope('loss'):\n",
    "        # MSE损失，将计算值回归到评分\n",
    "        cost = tf.losses.mean_squared_error(targets, inference)\n",
    "        loss = tf.reduce_sum(cost)\n",
    "    # 优化损失\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "    gradients = optimizer.compute_gradients(loss)\n",
    "    train_op = optimizer.apply_gradients(gradients, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'inference/ExpandDims:0' shape=(?, 1) dtype=float32>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 取得batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(Xs, ys, batch_size):\n",
    "    for start in range(0, len(Xs), batch_size):\n",
    "        end = min(start + batch_size, len(Xs))\n",
    "        yield Xs[start:end], ys[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to C:\\Users\\Administrator\\PycharmProjects\\pynb\\runs\\1544082093\n",
      "\n",
      "2018-12-06T15:41:34.472485: Epoch   0 Batch    0/3125 train_loss = 14.626\n",
      "2018-12-06T15:41:34.724809: Epoch   0 Batch   20/3125 train_loss = 14.653\n",
      "2018-12-06T15:41:34.975295: Epoch   0 Batch   40/3125 train_loss = 14.008\n",
      "2018-12-06T15:41:35.228249: Epoch   0 Batch   60/3125 train_loss = 13.170\n",
      "2018-12-06T15:41:35.482568: Epoch   0 Batch   80/3125 train_loss = 13.687\n",
      "2018-12-06T15:41:35.742871: Epoch   0 Batch  100/3125 train_loss = 12.749\n",
      "2018-12-06T15:41:35.994198: Epoch   0 Batch  120/3125 train_loss = 13.170\n",
      "2018-12-06T15:41:36.251917: Epoch   0 Batch  140/3125 train_loss = 12.457\n",
      "2018-12-06T15:41:36.503455: Epoch   0 Batch  160/3125 train_loss = 12.091\n",
      "2018-12-06T15:41:36.757901: Epoch   0 Batch  180/3125 train_loss = 11.430\n",
      "2018-12-06T15:41:37.011478: Epoch   0 Batch  200/3125 train_loss = 9.537\n",
      "2018-12-06T15:41:37.275770: Epoch   0 Batch  220/3125 train_loss = 9.043\n",
      "2018-12-06T15:41:37.616857: Epoch   0 Batch  240/3125 train_loss = 7.921\n",
      "2018-12-06T15:41:37.928547: Epoch   0 Batch  260/3125 train_loss = 6.955\n",
      "2018-12-06T15:41:38.202811: Epoch   0 Batch  280/3125 train_loss = 6.589\n",
      "2018-12-06T15:41:38.490043: Epoch   0 Batch  300/3125 train_loss = 5.578\n",
      "2018-12-06T15:41:38.745359: Epoch   0 Batch  320/3125 train_loss = 4.648\n",
      "2018-12-06T15:41:38.993695: Epoch   0 Batch  340/3125 train_loss = 4.213\n",
      "2018-12-06T15:41:39.253000: Epoch   0 Batch  360/3125 train_loss = 3.846\n",
      "2018-12-06T15:41:39.519287: Epoch   0 Batch  380/3125 train_loss = 3.073\n",
      "2018-12-06T15:41:39.792555: Epoch   0 Batch  400/3125 train_loss = 2.491\n",
      "2018-12-06T15:41:40.048872: Epoch   0 Batch  420/3125 train_loss = 1.939\n",
      "2018-12-06T15:41:40.312987: Epoch   0 Batch  440/3125 train_loss = 2.147\n",
      "2018-12-06T15:41:40.570427: Epoch   0 Batch  460/3125 train_loss = 1.944\n",
      "2018-12-06T15:41:40.828749: Epoch   0 Batch  480/3125 train_loss = 1.848\n",
      "2018-12-06T15:41:41.076087: Epoch   0 Batch  500/3125 train_loss = 1.436\n",
      "2018-12-06T15:41:41.336390: Epoch   0 Batch  520/3125 train_loss = 1.749\n",
      "2018-12-06T15:41:41.589220: Epoch   0 Batch  540/3125 train_loss = 1.581\n",
      "2018-12-06T15:41:41.869973: Epoch   0 Batch  560/3125 train_loss = 1.826\n",
      "2018-12-06T15:41:42.188142: Epoch   0 Batch  580/3125 train_loss = 1.829\n",
      "2018-12-06T15:41:42.455440: Epoch   0 Batch  600/3125 train_loss = 1.810\n",
      "2018-12-06T15:41:42.736687: Epoch   0 Batch  620/3125 train_loss = 1.745\n",
      "2018-12-06T15:41:43.003971: Epoch   0 Batch  640/3125 train_loss = 1.676\n",
      "2018-12-06T15:41:43.251309: Epoch   0 Batch  660/3125 train_loss = 1.560\n",
      "2018-12-06T15:41:43.499511: Epoch   0 Batch  680/3125 train_loss = 1.539\n",
      "2018-12-06T15:41:43.741370: Epoch   0 Batch  700/3125 train_loss = 1.588\n",
      "2018-12-06T15:41:43.988707: Epoch   0 Batch  720/3125 train_loss = 1.602\n",
      "2018-12-06T15:41:44.232056: Epoch   0 Batch  740/3125 train_loss = 1.690\n",
      "2018-12-06T15:41:44.477905: Epoch   0 Batch  760/3125 train_loss = 1.541\n",
      "2018-12-06T15:41:44.721253: Epoch   0 Batch  780/3125 train_loss = 1.614\n",
      "2018-12-06T15:41:44.966105: Epoch   0 Batch  800/3125 train_loss = 1.329\n",
      "2018-12-06T15:41:45.216452: Epoch   0 Batch  820/3125 train_loss = 1.464\n",
      "2018-12-06T15:41:45.468776: Epoch   0 Batch  840/3125 train_loss = 1.499\n",
      "2018-12-06T15:41:45.716114: Epoch   0 Batch  860/3125 train_loss = 1.410\n",
      "2018-12-06T15:41:45.962455: Epoch   0 Batch  880/3125 train_loss = 1.444\n",
      "2018-12-06T15:41:46.212784: Epoch   0 Batch  900/3125 train_loss = 1.412\n",
      "2018-12-06T15:41:46.476584: Epoch   0 Batch  920/3125 train_loss = 1.326\n",
      "2018-12-06T15:41:46.742871: Epoch   0 Batch  940/3125 train_loss = 1.422\n",
      "2018-12-06T15:41:46.987216: Epoch   0 Batch  960/3125 train_loss = 1.469\n",
      "2018-12-06T15:41:47.270459: Epoch   0 Batch  980/3125 train_loss = 1.436\n",
      "2018-12-06T15:41:47.550724: Epoch   0 Batch 1000/3125 train_loss = 1.445\n",
      "2018-12-06T15:41:47.810031: Epoch   0 Batch 1020/3125 train_loss = 1.653\n",
      "2018-12-06T15:41:48.061358: Epoch   0 Batch 1040/3125 train_loss = 1.402\n",
      "2018-12-06T15:41:48.313682: Epoch   0 Batch 1060/3125 train_loss = 1.456\n",
      "2018-12-06T15:41:48.557093: Epoch   0 Batch 1080/3125 train_loss = 1.177\n",
      "2018-12-06T15:41:48.802440: Epoch   0 Batch 1100/3125 train_loss = 1.532\n",
      "2018-12-06T15:41:49.051772: Epoch   0 Batch 1120/3125 train_loss = 1.480\n",
      "2018-12-06T15:41:49.297115: Epoch   0 Batch 1140/3125 train_loss = 1.413\n",
      "2018-12-06T15:41:49.547445: Epoch   0 Batch 1160/3125 train_loss = 1.334\n",
      "2018-12-06T15:41:49.804182: Epoch   0 Batch 1180/3125 train_loss = 1.334\n",
      "2018-12-06T15:41:50.054814: Epoch   0 Batch 1200/3125 train_loss = 1.455\n",
      "2018-12-06T15:41:50.319261: Epoch   0 Batch 1220/3125 train_loss = 1.367\n",
      "2018-12-06T15:41:50.572091: Epoch   0 Batch 1240/3125 train_loss = 1.163\n",
      "2018-12-06T15:41:50.846358: Epoch   0 Batch 1260/3125 train_loss = 1.340\n",
      "2018-12-06T15:41:51.117631: Epoch   0 Batch 1280/3125 train_loss = 1.394\n",
      "2018-12-06T15:41:51.368958: Epoch   0 Batch 1300/3125 train_loss = 1.379\n",
      "2018-12-06T15:41:51.672147: Epoch   0 Batch 1320/3125 train_loss = 1.347\n",
      "2018-12-06T15:41:51.948358: Epoch   0 Batch 1340/3125 train_loss = 1.220\n",
      "2018-12-06T15:41:52.229100: Epoch   0 Batch 1360/3125 train_loss = 1.259\n",
      "2018-12-06T15:41:52.508353: Epoch   0 Batch 1380/3125 train_loss = 1.138\n",
      "2018-12-06T15:41:52.788602: Epoch   0 Batch 1400/3125 train_loss = 1.325\n",
      "2018-12-06T15:41:53.047417: Epoch   0 Batch 1420/3125 train_loss = 1.320\n",
      "2018-12-06T15:41:53.300738: Epoch   0 Batch 1440/3125 train_loss = 1.197\n",
      "2018-12-06T15:41:53.554074: Epoch   0 Batch 1460/3125 train_loss = 1.320\n",
      "2018-12-06T15:41:53.798432: Epoch   0 Batch 1480/3125 train_loss = 1.433\n",
      "2018-12-06T15:41:54.045770: Epoch   0 Batch 1500/3125 train_loss = 1.357\n",
      "2018-12-06T15:41:54.294104: Epoch   0 Batch 1520/3125 train_loss = 1.370\n",
      "2018-12-06T15:41:54.543437: Epoch   0 Batch 1540/3125 train_loss = 1.316\n",
      "2018-12-06T15:41:54.789777: Epoch   0 Batch 1560/3125 train_loss = 1.334\n",
      "2018-12-06T15:41:55.033743: Epoch   0 Batch 1580/3125 train_loss = 1.371\n",
      "2018-12-06T15:41:55.277091: Epoch   0 Batch 1600/3125 train_loss = 1.319\n",
      "2018-12-06T15:41:55.530413: Epoch   0 Batch 1620/3125 train_loss = 1.304\n",
      "2018-12-06T15:41:55.771410: Epoch   0 Batch 1640/3125 train_loss = 1.462\n",
      "2018-12-06T15:41:56.017257: Epoch   0 Batch 1660/3125 train_loss = 1.348\n",
      "2018-12-06T15:41:56.267095: Epoch   0 Batch 1680/3125 train_loss = 1.280\n",
      "2018-12-06T15:41:56.513449: Epoch   0 Batch 1700/3125 train_loss = 1.116\n",
      "2018-12-06T15:41:56.756797: Epoch   0 Batch 1720/3125 train_loss = 1.316\n",
      "2018-12-06T15:41:57.071953: Epoch   0 Batch 1740/3125 train_loss = 1.310\n",
      "2018-12-06T15:41:57.354197: Epoch   0 Batch 1760/3125 train_loss = 1.463\n",
      "2018-12-06T15:41:57.628464: Epoch   0 Batch 1780/3125 train_loss = 1.235\n",
      "2018-12-06T15:41:57.907239: Epoch   0 Batch 1800/3125 train_loss = 1.163\n",
      "2018-12-06T15:41:58.177516: Epoch   0 Batch 1820/3125 train_loss = 1.264\n",
      "2018-12-06T15:41:58.425850: Epoch   0 Batch 1840/3125 train_loss = 1.383\n",
      "2018-12-06T15:41:58.674188: Epoch   0 Batch 1860/3125 train_loss = 1.260\n",
      "2018-12-06T15:41:58.917536: Epoch   0 Batch 1880/3125 train_loss = 1.339\n",
      "2018-12-06T15:41:59.176350: Epoch   0 Batch 1900/3125 train_loss = 1.092\n",
      "2018-12-06T15:41:59.428674: Epoch   0 Batch 1920/3125 train_loss = 1.189\n",
      "2018-12-06T15:41:59.683990: Epoch   0 Batch 1940/3125 train_loss = 1.173\n",
      "2018-12-06T15:41:59.949284: Epoch   0 Batch 1960/3125 train_loss = 1.202\n",
      "2018-12-06T15:42:00.234517: Epoch   0 Batch 1980/3125 train_loss = 1.237\n",
      "2018-12-06T15:42:00.506787: Epoch   0 Batch 2000/3125 train_loss = 1.503\n",
      "2018-12-06T15:42:00.769085: Epoch   0 Batch 2020/3125 train_loss = 1.410\n",
      "2018-12-06T15:42:01.030387: Epoch   0 Batch 2040/3125 train_loss = 1.217\n",
      "2018-12-06T15:42:01.332398: Epoch   0 Batch 2060/3125 train_loss = 1.046\n",
      "2018-12-06T15:42:01.595692: Epoch   0 Batch 2080/3125 train_loss = 1.412\n",
      "2018-12-06T15:42:01.850015: Epoch   0 Batch 2100/3125 train_loss = 1.176\n",
      "2018-12-06T15:42:02.158189: Epoch   0 Batch 2120/3125 train_loss = 1.234\n",
      "2018-12-06T15:42:02.439436: Epoch   0 Batch 2140/3125 train_loss = 1.223\n",
      "2018-12-06T15:42:02.719685: Epoch   0 Batch 2160/3125 train_loss = 1.223\n",
      "2018-12-06T15:42:02.973940: Epoch   0 Batch 2180/3125 train_loss = 1.226\n",
      "2018-12-06T15:42:03.226770: Epoch   0 Batch 2200/3125 train_loss = 1.139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-06T15:42:03.506545: Epoch   0 Batch 2220/3125 train_loss = 1.190\n",
      "2018-12-06T15:42:03.763855: Epoch   0 Batch 2240/3125 train_loss = 1.106\n",
      "2018-12-06T15:42:04.020169: Epoch   0 Batch 2260/3125 train_loss = 1.243\n",
      "2018-12-06T15:42:04.282467: Epoch   0 Batch 2280/3125 train_loss = 1.214\n",
      "2018-12-06T15:42:04.539777: Epoch   0 Batch 2300/3125 train_loss = 1.208\n",
      "2018-12-06T15:42:04.792360: Epoch   0 Batch 2320/3125 train_loss = 1.325\n",
      "2018-12-06T15:42:05.042056: Epoch   0 Batch 2340/3125 train_loss = 1.219\n",
      "2018-12-06T15:42:05.293383: Epoch   0 Batch 2360/3125 train_loss = 1.249\n",
      "2018-12-06T15:42:05.546704: Epoch   0 Batch 2380/3125 train_loss = 1.268\n",
      "2018-12-06T15:42:05.798032: Epoch   0 Batch 2400/3125 train_loss = 1.273\n",
      "2018-12-06T15:42:06.044372: Epoch   0 Batch 2420/3125 train_loss = 1.120\n",
      "2018-12-06T15:42:06.292365: Epoch   0 Batch 2440/3125 train_loss = 1.304\n",
      "2018-12-06T15:42:06.545209: Epoch   0 Batch 2460/3125 train_loss = 1.145\n",
      "2018-12-06T15:42:06.798037: Epoch   0 Batch 2480/3125 train_loss = 1.237\n",
      "2018-12-06T15:42:07.051359: Epoch   0 Batch 2500/3125 train_loss = 1.302\n",
      "2018-12-06T15:42:07.305678: Epoch   0 Batch 2520/3125 train_loss = 1.103\n",
      "2018-12-06T15:42:07.562386: Epoch   0 Batch 2540/3125 train_loss = 1.102\n",
      "2018-12-06T15:42:07.815977: Epoch   0 Batch 2560/3125 train_loss = 1.010\n",
      "2018-12-06T15:42:08.071294: Epoch   0 Batch 2580/3125 train_loss = 1.203\n",
      "2018-12-06T15:42:08.322621: Epoch   0 Batch 2600/3125 train_loss = 1.194\n",
      "2018-12-06T15:42:08.569959: Epoch   0 Batch 2620/3125 train_loss = 1.077\n",
      "2018-12-06T15:42:08.821259: Epoch   0 Batch 2640/3125 train_loss = 1.281\n",
      "2018-12-06T15:42:09.076632: Epoch   0 Batch 2660/3125 train_loss = 1.248\n",
      "2018-12-06T15:42:09.332960: Epoch   0 Batch 2680/3125 train_loss = 1.069\n",
      "2018-12-06T15:42:09.587279: Epoch   0 Batch 2700/3125 train_loss = 1.276\n",
      "2018-12-06T15:42:09.847583: Epoch   0 Batch 2720/3125 train_loss = 1.125\n",
      "2018-12-06T15:42:10.102899: Epoch   0 Batch 2740/3125 train_loss = 1.292\n",
      "2018-12-06T15:42:10.362204: Epoch   0 Batch 2760/3125 train_loss = 1.169\n",
      "2018-12-06T15:42:10.636470: Epoch   0 Batch 2780/3125 train_loss = 1.137\n",
      "2018-12-06T15:42:10.888807: Epoch   0 Batch 2800/3125 train_loss = 1.417\n",
      "2018-12-06T15:42:11.137659: Epoch   0 Batch 2820/3125 train_loss = 1.381\n",
      "2018-12-06T15:42:11.389983: Epoch   0 Batch 2840/3125 train_loss = 1.164\n",
      "2018-12-06T15:42:11.656270: Epoch   0 Batch 2860/3125 train_loss = 1.109\n",
      "2018-12-06T15:42:11.915576: Epoch   0 Batch 2880/3125 train_loss = 1.174\n",
      "2018-12-06T15:42:12.169894: Epoch   0 Batch 2900/3125 train_loss = 1.136\n",
      "2018-12-06T15:42:12.420931: Epoch   0 Batch 2920/3125 train_loss = 1.197\n",
      "2018-12-06T15:42:12.669283: Epoch   0 Batch 2940/3125 train_loss = 1.126\n",
      "2018-12-06T15:42:12.922604: Epoch   0 Batch 2960/3125 train_loss = 1.166\n",
      "2018-12-06T15:42:13.181910: Epoch   0 Batch 2980/3125 train_loss = 1.108\n",
      "2018-12-06T15:42:13.435232: Epoch   0 Batch 3000/3125 train_loss = 1.167\n",
      "2018-12-06T15:42:13.683567: Epoch   0 Batch 3020/3125 train_loss = 1.306\n",
      "2018-12-06T15:42:13.935657: Epoch   0 Batch 3040/3125 train_loss = 1.205\n",
      "2018-12-06T15:42:14.190681: Epoch   0 Batch 3060/3125 train_loss = 1.075\n",
      "2018-12-06T15:42:14.450983: Epoch   0 Batch 3080/3125 train_loss = 1.326\n",
      "2018-12-06T15:42:14.705303: Epoch   0 Batch 3100/3125 train_loss = 1.232\n",
      "2018-12-06T15:42:14.954635: Epoch   0 Batch 3120/3125 train_loss = 1.010\n",
      "2018-12-06T15:42:15.061350: Epoch   0 Batch    0/781 test_loss = 1.005\n",
      "2018-12-06T15:42:15.184153: Epoch   0 Batch   20/781 test_loss = 1.115\n",
      "2018-12-06T15:42:15.298845: Epoch   0 Batch   40/781 test_loss = 1.104\n",
      "2018-12-06T15:42:15.413823: Epoch   0 Batch   60/781 test_loss = 1.312\n",
      "2018-12-06T15:42:15.527027: Epoch   0 Batch   80/781 test_loss = 1.318\n",
      "2018-12-06T15:42:15.637730: Epoch   0 Batch  100/781 test_loss = 1.366\n",
      "2018-12-06T15:42:15.750429: Epoch   0 Batch  120/781 test_loss = 1.158\n",
      "2018-12-06T15:42:15.867116: Epoch   0 Batch  140/781 test_loss = 1.213\n",
      "2018-12-06T15:42:15.982806: Epoch   0 Batch  160/781 test_loss = 1.263\n",
      "2018-12-06T15:42:16.100491: Epoch   0 Batch  180/781 test_loss = 1.224\n",
      "2018-12-06T15:42:16.215185: Epoch   0 Batch  200/781 test_loss = 1.183\n",
      "2018-12-06T15:42:16.325888: Epoch   0 Batch  220/781 test_loss = 0.945\n",
      "2018-12-06T15:42:16.439583: Epoch   0 Batch  240/781 test_loss = 1.108\n",
      "2018-12-06T15:42:16.548292: Epoch   0 Batch  260/781 test_loss = 1.142\n",
      "2018-12-06T15:42:16.659993: Epoch   0 Batch  280/781 test_loss = 1.391\n",
      "2018-12-06T15:42:16.771694: Epoch   0 Batch  300/781 test_loss = 1.149\n",
      "2018-12-06T15:42:16.884709: Epoch   0 Batch  320/781 test_loss = 1.253\n",
      "2018-12-06T15:42:16.995412: Epoch   0 Batch  340/781 test_loss = 0.858\n",
      "2018-12-06T15:42:17.113098: Epoch   0 Batch  360/781 test_loss = 1.209\n",
      "2018-12-06T15:42:17.232777: Epoch   0 Batch  380/781 test_loss = 1.108\n",
      "2018-12-06T15:42:17.349464: Epoch   0 Batch  400/781 test_loss = 1.162\n",
      "2018-12-06T15:42:17.468147: Epoch   0 Batch  420/781 test_loss = 1.117\n",
      "2018-12-06T15:42:17.582840: Epoch   0 Batch  440/781 test_loss = 1.206\n",
      "2018-12-06T15:42:17.695214: Epoch   0 Batch  460/781 test_loss = 1.166\n",
      "2018-12-06T15:42:17.805917: Epoch   0 Batch  480/781 test_loss = 1.155\n",
      "2018-12-06T15:42:17.922605: Epoch   0 Batch  500/781 test_loss = 1.001\n",
      "2018-12-06T15:42:18.036300: Epoch   0 Batch  520/781 test_loss = 1.221\n",
      "2018-12-06T15:42:18.150911: Epoch   0 Batch  540/781 test_loss = 1.094\n",
      "2018-12-06T15:42:18.265603: Epoch   0 Batch  560/781 test_loss = 1.283\n",
      "2018-12-06T15:42:18.378809: Epoch   0 Batch  580/781 test_loss = 1.231\n",
      "2018-12-06T15:42:18.487517: Epoch   0 Batch  600/781 test_loss = 1.196\n",
      "2018-12-06T15:42:18.601212: Epoch   0 Batch  620/781 test_loss = 1.169\n",
      "2018-12-06T15:42:18.721889: Epoch   0 Batch  640/781 test_loss = 1.178\n",
      "2018-12-06T15:42:18.835585: Epoch   0 Batch  660/781 test_loss = 1.240\n",
      "2018-12-06T15:42:18.952272: Epoch   0 Batch  680/781 test_loss = 1.332\n",
      "2018-12-06T15:42:19.069957: Epoch   0 Batch  700/781 test_loss = 1.110\n",
      "2018-12-06T15:42:19.188639: Epoch   0 Batch  720/781 test_loss = 1.187\n",
      "2018-12-06T15:42:19.297348: Epoch   0 Batch  740/781 test_loss = 1.124\n",
      "2018-12-06T15:42:19.410046: Epoch   0 Batch  760/781 test_loss = 1.125\n",
      "2018-12-06T15:42:19.521367: Epoch   0 Batch  780/781 test_loss = 1.132\n",
      "2018-12-06T15:42:20.158685: Epoch   1 Batch   15/3125 train_loss = 1.205\n",
      "2018-12-06T15:42:20.416994: Epoch   1 Batch   35/3125 train_loss = 1.135\n",
      "2018-12-06T15:42:20.679291: Epoch   1 Batch   55/3125 train_loss = 1.290\n",
      "2018-12-06T15:42:20.928623: Epoch   1 Batch   75/3125 train_loss = 1.111\n",
      "2018-12-06T15:42:21.176959: Epoch   1 Batch   95/3125 train_loss = 1.055\n",
      "2018-12-06T15:42:21.427288: Epoch   1 Batch  115/3125 train_loss = 1.183\n",
      "2018-12-06T15:42:21.677618: Epoch   1 Batch  135/3125 train_loss = 0.994\n",
      "2018-12-06T15:42:21.931309: Epoch   1 Batch  155/3125 train_loss = 1.157\n",
      "2018-12-06T15:42:22.195601: Epoch   1 Batch  175/3125 train_loss = 1.071\n",
      "2018-12-06T15:42:22.452913: Epoch   1 Batch  195/3125 train_loss = 1.215\n",
      "2018-12-06T15:42:22.709734: Epoch   1 Batch  215/3125 train_loss = 1.112\n",
      "2018-12-06T15:42:22.961061: Epoch   1 Batch  235/3125 train_loss = 1.130\n",
      "2018-12-06T15:42:23.211390: Epoch   1 Batch  255/3125 train_loss = 1.230\n",
      "2018-12-06T15:42:23.463714: Epoch   1 Batch  275/3125 train_loss = 0.996\n",
      "2018-12-06T15:42:23.716177: Epoch   1 Batch  295/3125 train_loss = 0.977\n",
      "2018-12-06T15:42:23.965524: Epoch   1 Batch  315/3125 train_loss = 1.169\n",
      "2018-12-06T15:42:24.217848: Epoch   1 Batch  335/3125 train_loss = 1.033\n",
      "2018-12-06T15:42:24.469175: Epoch   1 Batch  355/3125 train_loss = 1.122\n",
      "2018-12-06T15:42:24.719505: Epoch   1 Batch  375/3125 train_loss = 1.186\n",
      "2018-12-06T15:42:24.977814: Epoch   1 Batch  395/3125 train_loss = 1.119\n",
      "2018-12-06T15:42:25.231135: Epoch   1 Batch  415/3125 train_loss = 1.253\n",
      "2018-12-06T15:42:25.476478: Epoch   1 Batch  435/3125 train_loss = 1.243\n",
      "2018-12-06T15:42:25.725811: Epoch   1 Batch  455/3125 train_loss = 1.168\n",
      "2018-12-06T15:42:25.976140: Epoch   1 Batch  475/3125 train_loss = 1.111\n",
      "2018-12-06T15:42:26.226470: Epoch   1 Batch  495/3125 train_loss = 1.035\n",
      "2018-12-06T15:42:26.476800: Epoch   1 Batch  515/3125 train_loss = 1.167\n",
      "2018-12-06T15:42:26.727129: Epoch   1 Batch  535/3125 train_loss = 1.229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-06T15:42:26.975971: Epoch   1 Batch  555/3125 train_loss = 1.290\n",
      "2018-12-06T15:42:27.232285: Epoch   1 Batch  575/3125 train_loss = 1.088\n",
      "2018-12-06T15:42:27.480619: Epoch   1 Batch  595/3125 train_loss = 1.325\n",
      "2018-12-06T15:42:27.724947: Epoch   1 Batch  615/3125 train_loss = 1.041\n",
      "2018-12-06T15:42:27.969292: Epoch   1 Batch  635/3125 train_loss = 1.140\n",
      "2018-12-06T15:42:28.235579: Epoch   1 Batch  655/3125 train_loss = 1.110\n",
      "2018-12-06T15:42:28.491894: Epoch   1 Batch  675/3125 train_loss = 0.932\n",
      "2018-12-06T15:42:28.746211: Epoch   1 Batch  695/3125 train_loss = 1.133\n",
      "2018-12-06T15:42:28.994354: Epoch   1 Batch  715/3125 train_loss = 1.095\n",
      "2018-12-06T15:42:29.246679: Epoch   1 Batch  735/3125 train_loss = 1.106\n",
      "2018-12-06T15:42:29.493426: Epoch   1 Batch  755/3125 train_loss = 1.168\n",
      "2018-12-06T15:42:29.740764: Epoch   1 Batch  775/3125 train_loss = 1.052\n",
      "2018-12-06T15:42:29.986107: Epoch   1 Batch  795/3125 train_loss = 1.145\n",
      "2018-12-06T15:42:30.240426: Epoch   1 Batch  815/3125 train_loss = 1.021\n",
      "2018-12-06T15:42:30.493718: Epoch   1 Batch  835/3125 train_loss = 1.035\n",
      "2018-12-06T15:42:30.743050: Epoch   1 Batch  855/3125 train_loss = 1.333\n",
      "2018-12-06T15:42:30.994329: Epoch   1 Batch  875/3125 train_loss = 1.208\n",
      "2018-12-06T15:42:31.242664: Epoch   1 Batch  895/3125 train_loss = 0.989\n",
      "2018-12-06T15:42:31.491000: Epoch   1 Batch  915/3125 train_loss = 1.157\n",
      "2018-12-06T15:42:31.737619: Epoch   1 Batch  935/3125 train_loss = 1.086\n",
      "2018-12-06T15:42:31.984956: Epoch   1 Batch  955/3125 train_loss = 1.172\n",
      "2018-12-06T15:42:32.233910: Epoch   1 Batch  975/3125 train_loss = 1.073\n",
      "2018-12-06T15:42:32.479253: Epoch   1 Batch  995/3125 train_loss = 0.870\n",
      "2018-12-06T15:42:32.730580: Epoch   1 Batch 1015/3125 train_loss = 1.136\n",
      "2018-12-06T15:42:32.977918: Epoch   1 Batch 1035/3125 train_loss = 1.088\n",
      "2018-12-06T15:42:33.228261: Epoch   1 Batch 1055/3125 train_loss = 1.127\n",
      "2018-12-06T15:42:33.480585: Epoch   1 Batch 1075/3125 train_loss = 1.081\n",
      "2018-12-06T15:42:33.728550: Epoch   1 Batch 1095/3125 train_loss = 0.986\n",
      "2018-12-06T15:42:33.980890: Epoch   1 Batch 1115/3125 train_loss = 1.149\n",
      "2018-12-06T15:42:34.240210: Epoch   1 Batch 1135/3125 train_loss = 1.130\n",
      "2018-12-06T15:42:34.486549: Epoch   1 Batch 1155/3125 train_loss = 1.116\n",
      "2018-12-06T15:42:34.736879: Epoch   1 Batch 1175/3125 train_loss = 1.088\n",
      "2018-12-06T15:42:34.994190: Epoch   1 Batch 1195/3125 train_loss = 1.236\n",
      "2018-12-06T15:42:35.247511: Epoch   1 Batch 1215/3125 train_loss = 0.980\n",
      "2018-12-06T15:42:35.497841: Epoch   1 Batch 1235/3125 train_loss = 1.144\n",
      "2018-12-06T15:42:35.753171: Epoch   1 Batch 1255/3125 train_loss = 0.956\n",
      "2018-12-06T15:42:36.002503: Epoch   1 Batch 1275/3125 train_loss = 1.046\n",
      "2018-12-06T15:42:36.254828: Epoch   1 Batch 1295/3125 train_loss = 1.064\n",
      "2018-12-06T15:42:36.501168: Epoch   1 Batch 1315/3125 train_loss = 1.281\n",
      "2018-12-06T15:42:36.752514: Epoch   1 Batch 1335/3125 train_loss = 1.068\n",
      "2018-12-06T15:42:37.000621: Epoch   1 Batch 1355/3125 train_loss = 1.059\n",
      "2018-12-06T15:42:37.254449: Epoch   1 Batch 1375/3125 train_loss = 1.154\n",
      "2018-12-06T15:42:37.506774: Epoch   1 Batch 1395/3125 train_loss = 1.074\n",
      "2018-12-06T15:42:37.760095: Epoch   1 Batch 1415/3125 train_loss = 1.132\n",
      "2018-12-06T15:42:38.010427: Epoch   1 Batch 1435/3125 train_loss = 1.120\n",
      "2018-12-06T15:42:38.268506: Epoch   1 Batch 1455/3125 train_loss = 1.172\n",
      "2018-12-06T15:42:38.516765: Epoch   1 Batch 1475/3125 train_loss = 1.143\n",
      "2018-12-06T15:42:38.765118: Epoch   1 Batch 1495/3125 train_loss = 1.086\n",
      "2018-12-06T15:42:39.013453: Epoch   1 Batch 1515/3125 train_loss = 1.080\n",
      "2018-12-06T15:42:39.267772: Epoch   1 Batch 1535/3125 train_loss = 0.953\n",
      "2018-12-06T15:42:39.514116: Epoch   1 Batch 1555/3125 train_loss = 1.168\n",
      "2018-12-06T15:42:39.764193: Epoch   1 Batch 1575/3125 train_loss = 1.080\n",
      "2018-12-06T15:42:40.015857: Epoch   1 Batch 1595/3125 train_loss = 1.133\n",
      "2018-12-06T15:42:40.272170: Epoch   1 Batch 1615/3125 train_loss = 1.103\n",
      "2018-12-06T15:42:40.525492: Epoch   1 Batch 1635/3125 train_loss = 1.103\n",
      "2018-12-06T15:42:40.773827: Epoch   1 Batch 1655/3125 train_loss = 1.139\n",
      "2018-12-06T15:42:41.030141: Epoch   1 Batch 1675/3125 train_loss = 1.016\n",
      "2018-12-06T15:42:41.275008: Epoch   1 Batch 1695/3125 train_loss = 1.088\n",
      "2018-12-06T15:42:41.527332: Epoch   1 Batch 1715/3125 train_loss = 0.972\n",
      "2018-12-06T15:42:41.775668: Epoch   1 Batch 1735/3125 train_loss = 1.180\n",
      "2018-12-06T15:42:42.026995: Epoch   1 Batch 1755/3125 train_loss = 1.030\n",
      "2018-12-06T15:42:42.275517: Epoch   1 Batch 1775/3125 train_loss = 1.077\n",
      "2018-12-06T15:42:42.528838: Epoch   1 Batch 1795/3125 train_loss = 1.034\n",
      "2018-12-06T15:42:42.778530: Epoch   1 Batch 1815/3125 train_loss = 1.084\n",
      "2018-12-06T15:42:43.027862: Epoch   1 Batch 1835/3125 train_loss = 1.176\n",
      "2018-12-06T15:42:43.281184: Epoch   1 Batch 1855/3125 train_loss = 1.019\n",
      "2018-12-06T15:42:43.532511: Epoch   1 Batch 1875/3125 train_loss = 1.129\n",
      "2018-12-06T15:42:43.783838: Epoch   1 Batch 1895/3125 train_loss = 1.071\n",
      "2018-12-06T15:42:44.033171: Epoch   1 Batch 1915/3125 train_loss = 0.970\n",
      "2018-12-06T15:42:44.287010: Epoch   1 Batch 1935/3125 train_loss = 1.085\n",
      "2018-12-06T15:42:44.538079: Epoch   1 Batch 1955/3125 train_loss = 0.988\n",
      "2018-12-06T15:42:44.790910: Epoch   1 Batch 1975/3125 train_loss = 1.063\n",
      "2018-12-06T15:42:45.038248: Epoch   1 Batch 1995/3125 train_loss = 1.249\n",
      "2018-12-06T15:42:45.292566: Epoch   1 Batch 2015/3125 train_loss = 1.140\n",
      "2018-12-06T15:42:45.543894: Epoch   1 Batch 2035/3125 train_loss = 1.118\n",
      "2018-12-06T15:42:45.801205: Epoch   1 Batch 2055/3125 train_loss = 0.932\n",
      "2018-12-06T15:42:46.051043: Epoch   1 Batch 2075/3125 train_loss = 1.198\n",
      "2018-12-06T15:42:46.303876: Epoch   1 Batch 2095/3125 train_loss = 1.002\n",
      "2018-12-06T15:42:46.563181: Epoch   1 Batch 2115/3125 train_loss = 1.139\n",
      "2018-12-06T15:42:46.820491: Epoch   1 Batch 2135/3125 train_loss = 1.046\n",
      "2018-12-06T15:42:47.072816: Epoch   1 Batch 2155/3125 train_loss = 1.053\n",
      "2018-12-06T15:42:47.326054: Epoch   1 Batch 2175/3125 train_loss = 1.084\n",
      "2018-12-06T15:42:47.580877: Epoch   1 Batch 2195/3125 train_loss = 1.062\n",
      "2018-12-06T15:42:47.828724: Epoch   1 Batch 2215/3125 train_loss = 1.056\n",
      "2018-12-06T15:42:48.081049: Epoch   1 Batch 2235/3125 train_loss = 1.165\n",
      "2018-12-06T15:42:48.336365: Epoch   1 Batch 2255/3125 train_loss = 1.117\n",
      "2018-12-06T15:42:48.589251: Epoch   1 Batch 2275/3125 train_loss = 0.916\n",
      "2018-12-06T15:42:48.846525: Epoch   1 Batch 2295/3125 train_loss = 1.352\n",
      "2018-12-06T15:42:49.100859: Epoch   1 Batch 2315/3125 train_loss = 1.239\n",
      "2018-12-06T15:42:49.358184: Epoch   1 Batch 2335/3125 train_loss = 1.086\n",
      "2018-12-06T15:42:49.607517: Epoch   1 Batch 2355/3125 train_loss = 1.125\n",
      "2018-12-06T15:42:49.859842: Epoch   1 Batch 2375/3125 train_loss = 1.205\n",
      "2018-12-06T15:42:50.115161: Epoch   1 Batch 2395/3125 train_loss = 1.033\n",
      "2018-12-06T15:42:50.375464: Epoch   1 Batch 2415/3125 train_loss = 1.091\n",
      "2018-12-06T15:42:50.628798: Epoch   1 Batch 2435/3125 train_loss = 1.042\n",
      "2018-12-06T15:42:50.880138: Epoch   1 Batch 2455/3125 train_loss = 1.149\n",
      "2018-12-06T15:42:51.137449: Epoch   1 Batch 2475/3125 train_loss = 1.035\n",
      "2018-12-06T15:42:51.398749: Epoch   1 Batch 2495/3125 train_loss = 0.951\n",
      "2018-12-06T15:42:51.657061: Epoch   1 Batch 2515/3125 train_loss = 1.109\n",
      "2018-12-06T15:42:51.911379: Epoch   1 Batch 2535/3125 train_loss = 1.174\n",
      "2018-12-06T15:42:52.164702: Epoch   1 Batch 2555/3125 train_loss = 0.883\n",
      "2018-12-06T15:42:52.419525: Epoch   1 Batch 2575/3125 train_loss = 0.952\n",
      "2018-12-06T15:42:52.670852: Epoch   1 Batch 2595/3125 train_loss = 1.039\n",
      "2018-12-06T15:42:52.921182: Epoch   1 Batch 2615/3125 train_loss = 1.132\n",
      "2018-12-06T15:42:53.177495: Epoch   1 Batch 2635/3125 train_loss = 1.041\n",
      "2018-12-06T15:42:53.428823: Epoch   1 Batch 2655/3125 train_loss = 1.095\n",
      "2018-12-06T15:42:53.677679: Epoch   1 Batch 2675/3125 train_loss = 0.961\n",
      "2018-12-06T15:42:53.923037: Epoch   1 Batch 2695/3125 train_loss = 1.133\n",
      "2018-12-06T15:42:54.175361: Epoch   1 Batch 2715/3125 train_loss = 1.066\n",
      "2018-12-06T15:42:54.427685: Epoch   1 Batch 2735/3125 train_loss = 0.943\n",
      "2018-12-06T15:42:54.681007: Epoch   1 Batch 2755/3125 train_loss = 1.101\n",
      "2018-12-06T15:42:54.934332: Epoch   1 Batch 2775/3125 train_loss = 1.096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-06T15:42:55.188960: Epoch   1 Batch 2795/3125 train_loss = 1.110\n",
      "2018-12-06T15:42:55.443788: Epoch   1 Batch 2815/3125 train_loss = 1.047\n",
      "2018-12-06T15:42:55.695114: Epoch   1 Batch 2835/3125 train_loss = 1.124\n",
      "2018-12-06T15:42:55.941467: Epoch   1 Batch 2855/3125 train_loss = 1.105\n",
      "2018-12-06T15:42:56.189793: Epoch   1 Batch 2875/3125 train_loss = 1.047\n",
      "2018-12-06T15:42:56.437638: Epoch   1 Batch 2895/3125 train_loss = 1.006\n",
      "2018-12-06T15:42:56.688452: Epoch   1 Batch 2915/3125 train_loss = 1.010\n",
      "2018-12-06T15:42:56.939780: Epoch   1 Batch 2935/3125 train_loss = 1.122\n",
      "2018-12-06T15:42:57.191107: Epoch   1 Batch 2955/3125 train_loss = 1.076\n",
      "2018-12-06T15:42:57.446927: Epoch   1 Batch 2975/3125 train_loss = 0.974\n",
      "2018-12-06T15:42:57.699252: Epoch   1 Batch 2995/3125 train_loss = 1.021\n",
      "2018-12-06T15:42:57.954041: Epoch   1 Batch 3015/3125 train_loss = 0.998\n",
      "2018-12-06T15:42:58.206366: Epoch   1 Batch 3035/3125 train_loss = 1.038\n",
      "2018-12-06T15:42:58.455699: Epoch   1 Batch 3055/3125 train_loss = 1.160\n",
      "2018-12-06T15:42:58.704034: Epoch   1 Batch 3075/3125 train_loss = 1.015\n",
      "2018-12-06T15:42:58.953366: Epoch   1 Batch 3095/3125 train_loss = 0.995\n",
      "2018-12-06T15:42:59.212671: Epoch   1 Batch 3115/3125 train_loss = 0.894\n",
      "2018-12-06T15:42:59.450036: Epoch   1 Batch   19/781 test_loss = 1.076\n",
      "2018-12-06T15:42:59.560248: Epoch   1 Batch   39/781 test_loss = 0.920\n",
      "2018-12-06T15:42:59.675447: Epoch   1 Batch   59/781 test_loss = 0.964\n",
      "2018-12-06T15:42:59.785154: Epoch   1 Batch   79/781 test_loss = 1.056\n",
      "2018-12-06T15:42:59.896854: Epoch   1 Batch   99/781 test_loss = 1.049\n",
      "2018-12-06T15:43:00.011547: Epoch   1 Batch  119/781 test_loss = 1.054\n",
      "2018-12-06T15:43:00.128235: Epoch   1 Batch  139/781 test_loss = 1.069\n",
      "2018-12-06T15:43:00.245920: Epoch   1 Batch  159/781 test_loss = 1.080\n",
      "2018-12-06T15:43:00.359615: Epoch   1 Batch  179/781 test_loss = 0.989\n",
      "2018-12-06T15:43:00.476535: Epoch   1 Batch  199/781 test_loss = 0.978\n",
      "2018-12-06T15:43:00.593223: Epoch   1 Batch  219/781 test_loss = 1.103\n",
      "2018-12-06T15:43:00.707917: Epoch   1 Batch  239/781 test_loss = 1.222\n",
      "2018-12-06T15:43:00.821621: Epoch   1 Batch  259/781 test_loss = 0.979\n",
      "2018-12-06T15:43:00.934310: Epoch   1 Batch  279/781 test_loss = 1.116\n",
      "2018-12-06T15:43:01.045519: Epoch   1 Batch  299/781 test_loss = 1.269\n",
      "2018-12-06T15:43:01.161717: Epoch   1 Batch  319/781 test_loss = 1.038\n",
      "2018-12-06T15:43:01.271929: Epoch   1 Batch  339/781 test_loss = 0.948\n",
      "2018-12-06T15:43:01.382142: Epoch   1 Batch  359/781 test_loss = 0.907\n",
      "2018-12-06T15:43:01.494840: Epoch   1 Batch  379/781 test_loss = 1.086\n",
      "2018-12-06T15:43:01.605544: Epoch   1 Batch  399/781 test_loss = 0.945\n",
      "2018-12-06T15:43:01.717244: Epoch   1 Batch  419/781 test_loss = 1.056\n",
      "2018-12-06T15:43:01.829943: Epoch   1 Batch  439/781 test_loss = 1.115\n",
      "2018-12-06T15:43:01.940647: Epoch   1 Batch  459/781 test_loss = 1.086\n",
      "2018-12-06T15:43:02.052347: Epoch   1 Batch  479/781 test_loss = 1.006\n",
      "2018-12-06T15:43:02.168037: Epoch   1 Batch  499/781 test_loss = 0.968\n",
      "2018-12-06T15:43:02.277744: Epoch   1 Batch  519/781 test_loss = 1.069\n",
      "2018-12-06T15:43:02.390587: Epoch   1 Batch  539/781 test_loss = 0.922\n",
      "2018-12-06T15:43:02.502288: Epoch   1 Batch  559/781 test_loss = 1.124\n",
      "2018-12-06T15:43:02.610997: Epoch   1 Batch  579/781 test_loss = 1.134\n",
      "2018-12-06T15:43:02.723695: Epoch   1 Batch  599/781 test_loss = 1.024\n",
      "2018-12-06T15:43:02.835396: Epoch   1 Batch  619/781 test_loss = 1.211\n",
      "2018-12-06T15:43:02.946099: Epoch   1 Batch  639/781 test_loss = 0.936\n",
      "2018-12-06T15:43:03.054809: Epoch   1 Batch  659/781 test_loss = 1.211\n",
      "2018-12-06T15:43:03.177480: Epoch   1 Batch  679/781 test_loss = 1.156\n",
      "2018-12-06T15:43:03.292681: Epoch   1 Batch  699/781 test_loss = 0.889\n",
      "2018-12-06T15:43:03.408373: Epoch   1 Batch  719/781 test_loss = 1.009\n",
      "2018-12-06T15:43:03.525058: Epoch   1 Batch  739/781 test_loss = 0.957\n",
      "2018-12-06T15:43:03.640748: Epoch   1 Batch  759/781 test_loss = 0.985\n",
      "2018-12-06T15:43:03.753447: Epoch   1 Batch  779/781 test_loss = 0.806\n",
      "2018-12-06T15:43:04.342193: Epoch   2 Batch   10/3125 train_loss = 1.013\n",
      "2018-12-06T15:43:04.592523: Epoch   2 Batch   30/3125 train_loss = 1.062\n",
      "2018-12-06T15:43:04.838864: Epoch   2 Batch   50/3125 train_loss = 1.131\n",
      "2018-12-06T15:43:05.089194: Epoch   2 Batch   70/3125 train_loss = 1.066\n",
      "2018-12-06T15:43:05.341354: Epoch   2 Batch   90/3125 train_loss = 1.039\n",
      "2018-12-06T15:43:05.589690: Epoch   2 Batch  110/3125 train_loss = 0.987\n",
      "2018-12-06T15:43:05.840019: Epoch   2 Batch  130/3125 train_loss = 1.037\n",
      "2018-12-06T15:43:06.086360: Epoch   2 Batch  150/3125 train_loss = 1.131\n",
      "2018-12-06T15:43:06.336286: Epoch   2 Batch  170/3125 train_loss = 0.994\n",
      "2018-12-06T15:43:06.587613: Epoch   2 Batch  190/3125 train_loss = 1.026\n",
      "2018-12-06T15:43:06.834963: Epoch   2 Batch  210/3125 train_loss = 1.062\n",
      "2018-12-06T15:43:07.092781: Epoch   2 Batch  230/3125 train_loss = 1.121\n",
      "2018-12-06T15:43:07.351088: Epoch   2 Batch  250/3125 train_loss = 0.937\n",
      "2018-12-06T15:43:07.603414: Epoch   2 Batch  270/3125 train_loss = 0.849\n",
      "2018-12-06T15:43:07.853213: Epoch   2 Batch  290/3125 train_loss = 1.085\n",
      "2018-12-06T15:43:08.108529: Epoch   2 Batch  310/3125 train_loss = 1.002\n",
      "2018-12-06T15:43:08.376828: Epoch   2 Batch  330/3125 train_loss = 1.041\n",
      "2018-12-06T15:43:08.629167: Epoch   2 Batch  350/3125 train_loss = 0.900\n",
      "2018-12-06T15:43:08.889470: Epoch   2 Batch  370/3125 train_loss = 1.136\n",
      "2018-12-06T15:43:09.160744: Epoch   2 Batch  390/3125 train_loss = 1.177\n",
      "2018-12-06T15:43:09.413418: Epoch   2 Batch  410/3125 train_loss = 0.930\n",
      "2018-12-06T15:43:09.659759: Epoch   2 Batch  430/3125 train_loss = 1.215\n",
      "2018-12-06T15:43:09.909784: Epoch   2 Batch  450/3125 train_loss = 0.980\n",
      "2018-12-06T15:43:10.167602: Epoch   2 Batch  470/3125 train_loss = 0.970\n",
      "2018-12-06T15:43:10.426908: Epoch   2 Batch  490/3125 train_loss = 1.143\n",
      "2018-12-06T15:43:10.678234: Epoch   2 Batch  510/3125 train_loss = 1.135\n",
      "2018-12-06T15:43:10.929069: Epoch   2 Batch  530/3125 train_loss = 1.058\n",
      "2018-12-06T15:43:11.181970: Epoch   2 Batch  550/3125 train_loss = 1.008\n",
      "2018-12-06T15:43:11.434295: Epoch   2 Batch  570/3125 train_loss = 1.143\n",
      "2018-12-06T15:43:11.684641: Epoch   2 Batch  590/3125 train_loss = 1.059\n",
      "2018-12-06T15:43:11.939958: Epoch   2 Batch  610/3125 train_loss = 0.942\n",
      "2018-12-06T15:43:12.191284: Epoch   2 Batch  630/3125 train_loss = 1.097\n",
      "2018-12-06T15:43:12.442612: Epoch   2 Batch  650/3125 train_loss = 1.066\n",
      "2018-12-06T15:43:12.696930: Epoch   2 Batch  670/3125 train_loss = 1.019\n",
      "2018-12-06T15:43:12.953759: Epoch   2 Batch  690/3125 train_loss = 0.993\n",
      "2018-12-06T15:43:13.215062: Epoch   2 Batch  710/3125 train_loss = 0.969\n",
      "2018-12-06T15:43:13.468384: Epoch   2 Batch  730/3125 train_loss = 0.897\n",
      "2018-12-06T15:43:13.718713: Epoch   2 Batch  750/3125 train_loss = 1.027\n",
      "2018-12-06T15:43:13.974033: Epoch   2 Batch  770/3125 train_loss = 0.865\n",
      "2018-12-06T15:43:14.249166: Epoch   2 Batch  790/3125 train_loss = 0.971\n",
      "2018-12-06T15:43:14.518578: Epoch   2 Batch  810/3125 train_loss = 0.878\n",
      "2018-12-06T15:43:14.789267: Epoch   2 Batch  830/3125 train_loss = 0.841\n",
      "2018-12-06T15:43:15.072510: Epoch   2 Batch  850/3125 train_loss = 1.008\n",
      "2018-12-06T15:43:15.403955: Epoch   2 Batch  870/3125 train_loss = 0.913\n",
      "2018-12-06T15:43:15.704658: Epoch   2 Batch  890/3125 train_loss = 0.947\n",
      "2018-12-06T15:43:15.973939: Epoch   2 Batch  910/3125 train_loss = 0.979\n",
      "2018-12-06T15:43:16.232753: Epoch   2 Batch  930/3125 train_loss = 1.057\n",
      "2018-12-06T15:43:16.483083: Epoch   2 Batch  950/3125 train_loss = 0.975\n",
      "2018-12-06T15:43:16.737401: Epoch   2 Batch  970/3125 train_loss = 1.072\n",
      "2018-12-06T15:43:16.993808: Epoch   2 Batch  990/3125 train_loss = 0.883\n",
      "2018-12-06T15:43:17.258988: Epoch   2 Batch 1010/3125 train_loss = 1.187\n",
      "2018-12-06T15:43:17.608016: Epoch   2 Batch 1030/3125 train_loss = 0.928\n",
      "2018-12-06T15:43:17.899236: Epoch   2 Batch 1050/3125 train_loss = 1.028\n",
      "2018-12-06T15:43:18.329085: Epoch   2 Batch 1070/3125 train_loss = 1.043\n",
      "2018-12-06T15:43:18.663701: Epoch   2 Batch 1090/3125 train_loss = 1.086\n",
      "2018-12-06T15:43:18.965908: Epoch   2 Batch 1110/3125 train_loss = 1.103\n",
      "2018-12-06T15:43:19.292035: Epoch   2 Batch 1130/3125 train_loss = 0.948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-06T15:43:19.586247: Epoch   2 Batch 1150/3125 train_loss = 0.999\n",
      "2018-12-06T15:43:19.868492: Epoch   2 Batch 1170/3125 train_loss = 1.024\n",
      "2018-12-06T15:43:20.144752: Epoch   2 Batch 1190/3125 train_loss = 1.058\n",
      "2018-12-06T15:43:20.438984: Epoch   2 Batch 1210/3125 train_loss = 0.923\n",
      "2018-12-06T15:43:20.746161: Epoch   2 Batch 1230/3125 train_loss = 0.885\n",
      "2018-12-06T15:43:21.014443: Epoch   2 Batch 1250/3125 train_loss = 0.988\n",
      "2018-12-06T15:43:21.273749: Epoch   2 Batch 1270/3125 train_loss = 1.096\n",
      "2018-12-06T15:43:21.532680: Epoch   2 Batch 1290/3125 train_loss = 0.904\n",
      "2018-12-06T15:43:21.792996: Epoch   2 Batch 1310/3125 train_loss = 1.038\n",
      "2018-12-06T15:43:22.041837: Epoch   2 Batch 1330/3125 train_loss = 1.089\n",
      "2018-12-06T15:43:22.293165: Epoch   2 Batch 1350/3125 train_loss = 0.913\n",
      "2018-12-06T15:43:22.538507: Epoch   2 Batch 1370/3125 train_loss = 0.844\n",
      "2018-12-06T15:43:22.783851: Epoch   2 Batch 1390/3125 train_loss = 1.085\n",
      "2018-12-06T15:43:23.030413: Epoch   2 Batch 1410/3125 train_loss = 1.024\n",
      "2018-12-06T15:43:23.284256: Epoch   2 Batch 1430/3125 train_loss = 1.021\n",
      "2018-12-06T15:43:23.529599: Epoch   2 Batch 1450/3125 train_loss = 1.069\n",
      "2018-12-06T15:43:23.774942: Epoch   2 Batch 1470/3125 train_loss = 1.011\n",
      "2018-12-06T15:43:24.021283: Epoch   2 Batch 1490/3125 train_loss = 1.038\n",
      "2018-12-06T15:43:24.278594: Epoch   2 Batch 1510/3125 train_loss = 0.946\n",
      "2018-12-06T15:43:24.543553: Epoch   2 Batch 1530/3125 train_loss = 1.103\n",
      "2018-12-06T15:43:24.799867: Epoch   2 Batch 1550/3125 train_loss = 0.878\n",
      "2018-12-06T15:43:25.049200: Epoch   2 Batch 1570/3125 train_loss = 0.960\n",
      "2018-12-06T15:43:25.302521: Epoch   2 Batch 1590/3125 train_loss = 1.019\n",
      "2018-12-06T15:43:25.585764: Epoch   2 Batch 1610/3125 train_loss = 0.961\n",
      "2018-12-06T15:43:25.852051: Epoch   2 Batch 1630/3125 train_loss = 1.054\n",
      "2018-12-06T15:43:26.112865: Epoch   2 Batch 1650/3125 train_loss = 0.906\n",
      "2018-12-06T15:43:26.396106: Epoch   2 Batch 1670/3125 train_loss = 0.872\n",
      "2018-12-06T15:43:26.649428: Epoch   2 Batch 1690/3125 train_loss = 1.084\n",
      "2018-12-06T15:43:26.903261: Epoch   2 Batch 1710/3125 train_loss = 0.953\n",
      "2018-12-06T15:43:27.213425: Epoch   2 Batch 1730/3125 train_loss = 1.001\n",
      "2018-12-06T15:43:27.482537: Epoch   2 Batch 1750/3125 train_loss = 0.899\n",
      "2018-12-06T15:43:27.828209: Epoch   2 Batch 1770/3125 train_loss = 1.119\n",
      "2018-12-06T15:43:28.111450: Epoch   2 Batch 1790/3125 train_loss = 1.055\n",
      "2018-12-06T15:43:28.399678: Epoch   2 Batch 1810/3125 train_loss = 1.045\n",
      "2018-12-06T15:43:28.679928: Epoch   2 Batch 1830/3125 train_loss = 1.062\n",
      "2018-12-06T15:43:28.957186: Epoch   2 Batch 1850/3125 train_loss = 0.969\n",
      "2018-12-06T15:43:29.231971: Epoch   2 Batch 1870/3125 train_loss = 1.061\n",
      "2018-12-06T15:43:29.485796: Epoch   2 Batch 1890/3125 train_loss = 0.857\n",
      "2018-12-06T15:43:29.735129: Epoch   2 Batch 1910/3125 train_loss = 0.950\n",
      "2018-12-06T15:43:29.981469: Epoch   2 Batch 1930/3125 train_loss = 0.970\n",
      "2018-12-06T15:43:30.231914: Epoch   2 Batch 1950/3125 train_loss = 0.888\n",
      "2018-12-06T15:43:30.478006: Epoch   2 Batch 1970/3125 train_loss = 1.003\n",
      "2018-12-06T15:43:30.731831: Epoch   2 Batch 1990/3125 train_loss = 0.881\n",
      "2018-12-06T15:43:31.013079: Epoch   2 Batch 2010/3125 train_loss = 0.796\n",
      "2018-12-06T15:43:31.269393: Epoch   2 Batch 2030/3125 train_loss = 1.031\n",
      "2018-12-06T15:43:31.526686: Epoch   2 Batch 2050/3125 train_loss = 0.987\n",
      "2018-12-06T15:43:31.803943: Epoch   2 Batch 2070/3125 train_loss = 0.897\n",
      "2018-12-06T15:43:32.059273: Epoch   2 Batch 2090/3125 train_loss = 0.916\n",
      "2018-12-06T15:43:32.316584: Epoch   2 Batch 2110/3125 train_loss = 1.021\n",
      "2018-12-06T15:43:32.571900: Epoch   2 Batch 2130/3125 train_loss = 0.938\n",
      "2018-12-06T15:43:32.823227: Epoch   2 Batch 2150/3125 train_loss = 0.976\n",
      "2018-12-06T15:43:33.084869: Epoch   2 Batch 2170/3125 train_loss = 0.878\n",
      "2018-12-06T15:43:33.377579: Epoch   2 Batch 2190/3125 train_loss = 0.942\n",
      "2018-12-06T15:43:33.677789: Epoch   2 Batch 2210/3125 train_loss = 1.012\n",
      "2018-12-06T15:43:33.940070: Epoch   2 Batch 2230/3125 train_loss = 0.902\n",
      "2018-12-06T15:43:34.195386: Epoch   2 Batch 2250/3125 train_loss = 1.032\n",
      "2018-12-06T15:43:34.445717: Epoch   2 Batch 2270/3125 train_loss = 0.941\n",
      "2018-12-06T15:43:34.704025: Epoch   2 Batch 2290/3125 train_loss = 0.852\n",
      "2018-12-06T15:43:34.953357: Epoch   2 Batch 2310/3125 train_loss = 0.970\n",
      "2018-12-06T15:43:35.215165: Epoch   2 Batch 2330/3125 train_loss = 1.067\n",
      "2018-12-06T15:43:35.471000: Epoch   2 Batch 2350/3125 train_loss = 1.113\n",
      "2018-12-06T15:43:35.764721: Epoch   2 Batch 2370/3125 train_loss = 0.943\n",
      "2018-12-06T15:43:36.025024: Epoch   2 Batch 2390/3125 train_loss = 1.076\n",
      "2018-12-06T15:43:36.282335: Epoch   2 Batch 2410/3125 train_loss = 1.098\n",
      "2018-12-06T15:43:36.531175: Epoch   2 Batch 2430/3125 train_loss = 0.962\n",
      "2018-12-06T15:43:36.786494: Epoch   2 Batch 2450/3125 train_loss = 0.995\n",
      "2018-12-06T15:43:37.047795: Epoch   2 Batch 2470/3125 train_loss = 1.018\n",
      "2018-12-06T15:43:37.311089: Epoch   2 Batch 2490/3125 train_loss = 1.071\n",
      "2018-12-06T15:43:37.582363: Epoch   2 Batch 2510/3125 train_loss = 1.158\n",
      "2018-12-06T15:43:37.831696: Epoch   2 Batch 2530/3125 train_loss = 0.806\n",
      "2018-12-06T15:43:38.085018: Epoch   2 Batch 2550/3125 train_loss = 1.024\n",
      "2018-12-06T15:43:38.340459: Epoch   2 Batch 2570/3125 train_loss = 0.988\n",
      "2018-12-06T15:43:38.595693: Epoch   2 Batch 2590/3125 train_loss = 1.054\n",
      "2018-12-06T15:43:38.854998: Epoch   2 Batch 2610/3125 train_loss = 1.030\n",
      "2018-12-06T15:43:39.108320: Epoch   2 Batch 2630/3125 train_loss = 0.730\n",
      "2018-12-06T15:43:39.360153: Epoch   2 Batch 2650/3125 train_loss = 0.992\n",
      "2018-12-06T15:43:39.619458: Epoch   2 Batch 2670/3125 train_loss = 1.029\n",
      "2018-12-06T15:43:39.871291: Epoch   2 Batch 2690/3125 train_loss = 1.019\n",
      "2018-12-06T15:43:40.120145: Epoch   2 Batch 2710/3125 train_loss = 0.870\n",
      "2018-12-06T15:43:40.379451: Epoch   2 Batch 2730/3125 train_loss = 1.099\n",
      "2018-12-06T15:43:40.631775: Epoch   2 Batch 2750/3125 train_loss = 1.027\n",
      "2018-12-06T15:43:40.885096: Epoch   2 Batch 2770/3125 train_loss = 0.961\n",
      "2018-12-06T15:43:41.142568: Epoch   2 Batch 2790/3125 train_loss = 0.960\n",
      "2018-12-06T15:43:41.399214: Epoch   2 Batch 2810/3125 train_loss = 0.960\n",
      "2018-12-06T15:43:41.674477: Epoch   2 Batch 2830/3125 train_loss = 0.828\n",
      "2018-12-06T15:43:41.932785: Epoch   2 Batch 2850/3125 train_loss = 0.956\n",
      "2018-12-06T15:43:42.185113: Epoch   2 Batch 2870/3125 train_loss = 0.833\n",
      "2018-12-06T15:43:42.440429: Epoch   2 Batch 2890/3125 train_loss = 0.795\n",
      "2018-12-06T15:43:42.692262: Epoch   2 Batch 2910/3125 train_loss = 1.024\n",
      "2018-12-06T15:43:42.947105: Epoch   2 Batch 2930/3125 train_loss = 0.871\n",
      "2018-12-06T15:43:43.207407: Epoch   2 Batch 2950/3125 train_loss = 1.081\n",
      "2018-12-06T15:43:43.453747: Epoch   2 Batch 2970/3125 train_loss = 1.053\n",
      "2018-12-06T15:43:43.705075: Epoch   2 Batch 2990/3125 train_loss = 0.905\n",
      "2018-12-06T15:43:43.956402: Epoch   2 Batch 3010/3125 train_loss = 0.979\n",
      "2018-12-06T15:43:44.210310: Epoch   2 Batch 3030/3125 train_loss = 1.048\n",
      "2018-12-06T15:43:44.462144: Epoch   2 Batch 3050/3125 train_loss = 0.946\n",
      "2018-12-06T15:43:44.713471: Epoch   2 Batch 3070/3125 train_loss = 0.911\n",
      "2018-12-06T15:43:44.969785: Epoch   2 Batch 3090/3125 train_loss = 0.866\n",
      "2018-12-06T15:43:45.235075: Epoch   2 Batch 3110/3125 train_loss = 0.900\n",
      "2018-12-06T15:43:45.519313: Epoch   2 Batch   18/781 test_loss = 0.839\n",
      "2018-12-06T15:43:45.627888: Epoch   2 Batch   38/781 test_loss = 0.952\n",
      "2018-12-06T15:43:45.738592: Epoch   2 Batch   58/781 test_loss = 0.915\n",
      "2018-12-06T15:43:45.849296: Epoch   2 Batch   78/781 test_loss = 0.916\n",
      "2018-12-06T15:43:45.959002: Epoch   2 Batch   98/781 test_loss = 0.957\n",
      "2018-12-06T15:43:46.071700: Epoch   2 Batch  118/781 test_loss = 0.828\n",
      "2018-12-06T15:43:46.187390: Epoch   2 Batch  138/781 test_loss = 1.076\n",
      "2018-12-06T15:43:46.303080: Epoch   2 Batch  158/781 test_loss = 0.885\n",
      "2018-12-06T15:43:46.411790: Epoch   2 Batch  178/781 test_loss = 0.848\n",
      "2018-12-06T15:43:46.524487: Epoch   2 Batch  198/781 test_loss = 0.954\n",
      "2018-12-06T15:43:46.637186: Epoch   2 Batch  218/781 test_loss = 1.095\n",
      "2018-12-06T15:43:46.748887: Epoch   2 Batch  238/781 test_loss = 0.959\n",
      "2018-12-06T15:43:46.860513: Epoch   2 Batch  258/781 test_loss = 1.012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-06T15:43:46.974209: Epoch   2 Batch  278/781 test_loss = 1.071\n",
      "2018-12-06T15:43:47.087905: Epoch   2 Batch  298/781 test_loss = 0.961\n",
      "2018-12-06T15:43:47.203594: Epoch   2 Batch  318/781 test_loss = 0.898\n",
      "2018-12-06T15:43:47.323275: Epoch   2 Batch  338/781 test_loss = 0.931\n",
      "2018-12-06T15:43:47.439962: Epoch   2 Batch  358/781 test_loss = 0.929\n",
      "2018-12-06T15:43:47.555653: Epoch   2 Batch  378/781 test_loss = 0.962\n",
      "2018-12-06T15:43:47.672342: Epoch   2 Batch  398/781 test_loss = 0.908\n",
      "2018-12-06T15:43:47.786389: Epoch   2 Batch  418/781 test_loss = 1.006\n",
      "2018-12-06T15:43:47.897093: Epoch   2 Batch  438/781 test_loss = 1.057\n",
      "2018-12-06T15:43:48.008794: Epoch   2 Batch  458/781 test_loss = 0.981\n",
      "2018-12-06T15:43:48.118500: Epoch   2 Batch  478/781 test_loss = 0.977\n",
      "2018-12-06T15:43:48.235779: Epoch   2 Batch  498/781 test_loss = 0.825\n",
      "2018-12-06T15:43:48.345998: Epoch   2 Batch  518/781 test_loss = 0.921\n",
      "2018-12-06T15:43:48.459193: Epoch   2 Batch  538/781 test_loss = 0.921\n",
      "2018-12-06T15:43:48.569898: Epoch   2 Batch  558/781 test_loss = 0.927\n",
      "2018-12-06T15:43:48.681598: Epoch   2 Batch  578/781 test_loss = 0.934\n",
      "2018-12-06T15:43:48.791304: Epoch   2 Batch  598/781 test_loss = 1.042\n",
      "2018-12-06T15:43:48.905000: Epoch   2 Batch  618/781 test_loss = 0.819\n",
      "2018-12-06T15:43:49.015704: Epoch   2 Batch  638/781 test_loss = 0.892\n",
      "2018-12-06T15:43:49.125410: Epoch   2 Batch  658/781 test_loss = 1.050\n",
      "2018-12-06T15:43:49.238792: Epoch   2 Batch  678/781 test_loss = 0.948\n",
      "2018-12-06T15:43:49.357986: Epoch   2 Batch  698/781 test_loss = 0.938\n",
      "2018-12-06T15:43:49.471680: Epoch   2 Batch  718/781 test_loss = 1.037\n",
      "2018-12-06T15:43:49.579391: Epoch   2 Batch  738/781 test_loss = 0.895\n",
      "2018-12-06T15:43:49.690098: Epoch   2 Batch  758/781 test_loss = 1.021\n",
      "2018-12-06T15:43:49.798315: Epoch   2 Batch  778/781 test_loss = 0.961\n",
      "2018-12-06T15:43:50.329047: Epoch   3 Batch    5/3125 train_loss = 0.970\n",
      "2018-12-06T15:43:50.578379: Epoch   3 Batch   25/3125 train_loss = 0.987\n",
      "2018-12-06T15:43:50.827712: Epoch   3 Batch   45/3125 train_loss = 0.912\n",
      "2018-12-06T15:43:51.073055: Epoch   3 Batch   65/3125 train_loss = 1.017\n",
      "2018-12-06T15:43:51.325380: Epoch   3 Batch   85/3125 train_loss = 0.885\n",
      "2018-12-06T15:43:51.577330: Epoch   3 Batch  105/3125 train_loss = 0.783\n",
      "2018-12-06T15:43:51.827539: Epoch   3 Batch  125/3125 train_loss = 0.921\n",
      "2018-12-06T15:43:52.078867: Epoch   3 Batch  145/3125 train_loss = 0.996\n",
      "2018-12-06T15:43:52.336177: Epoch   3 Batch  165/3125 train_loss = 0.991\n",
      "2018-12-06T15:43:52.589497: Epoch   3 Batch  185/3125 train_loss = 0.893\n",
      "2018-12-06T15:43:52.837832: Epoch   3 Batch  205/3125 train_loss = 0.820\n",
      "2018-12-06T15:43:53.086206: Epoch   3 Batch  225/3125 train_loss = 0.834\n",
      "2018-12-06T15:43:53.339528: Epoch   3 Batch  245/3125 train_loss = 1.123\n",
      "2018-12-06T15:43:53.592849: Epoch   3 Batch  265/3125 train_loss = 0.895\n",
      "2018-12-06T15:43:53.842182: Epoch   3 Batch  285/3125 train_loss = 1.042\n",
      "2018-12-06T15:43:54.097172: Epoch   3 Batch  305/3125 train_loss = 0.906\n",
      "2018-12-06T15:43:54.409336: Epoch   3 Batch  325/3125 train_loss = 0.962\n",
      "2018-12-06T15:43:54.669639: Epoch   3 Batch  345/3125 train_loss = 0.972\n",
      "2018-12-06T15:43:54.920966: Epoch   3 Batch  365/3125 train_loss = 0.907\n",
      "2018-12-06T15:43:55.197225: Epoch   3 Batch  385/3125 train_loss = 0.905\n",
      "2018-12-06T15:43:55.445561: Epoch   3 Batch  405/3125 train_loss = 0.891\n",
      "2018-12-06T15:43:55.695692: Epoch   3 Batch  425/3125 train_loss = 1.019\n",
      "2018-12-06T15:43:55.950007: Epoch   3 Batch  445/3125 train_loss = 0.959\n",
      "2018-12-06T15:43:56.200337: Epoch   3 Batch  465/3125 train_loss = 0.909\n",
      "2018-12-06T15:43:56.445679: Epoch   3 Batch  485/3125 train_loss = 1.115\n",
      "2018-12-06T15:43:56.701993: Epoch   3 Batch  505/3125 train_loss = 0.912\n",
      "2018-12-06T15:43:56.954318: Epoch   3 Batch  525/3125 train_loss = 0.996\n",
      "2018-12-06T15:43:57.209637: Epoch   3 Batch  545/3125 train_loss = 0.846\n",
      "2018-12-06T15:43:57.463463: Epoch   3 Batch  565/3125 train_loss = 1.078\n",
      "2018-12-06T15:43:57.712796: Epoch   3 Batch  585/3125 train_loss = 0.912\n",
      "2018-12-06T15:43:57.962634: Epoch   3 Batch  605/3125 train_loss = 0.949\n",
      "2018-12-06T15:43:58.216461: Epoch   3 Batch  625/3125 train_loss = 0.974\n",
      "2018-12-06T15:43:58.469786: Epoch   3 Batch  645/3125 train_loss = 1.022\n",
      "2018-12-06T15:43:58.717124: Epoch   3 Batch  665/3125 train_loss = 1.034\n",
      "2018-12-06T15:43:58.974435: Epoch   3 Batch  685/3125 train_loss = 0.929\n",
      "2018-12-06T15:43:59.236733: Epoch   3 Batch  705/3125 train_loss = 1.087\n",
      "2018-12-06T15:43:59.492776: Epoch   3 Batch  725/3125 train_loss = 0.982\n",
      "2018-12-06T15:43:59.743109: Epoch   3 Batch  745/3125 train_loss = 0.957\n",
      "2018-12-06T15:43:59.991395: Epoch   3 Batch  765/3125 train_loss = 0.938\n",
      "2018-12-06T15:44:00.260687: Epoch   3 Batch  785/3125 train_loss = 1.099\n",
      "2018-12-06T15:44:00.510020: Epoch   3 Batch  805/3125 train_loss = 0.871\n",
      "2018-12-06T15:44:00.761347: Epoch   3 Batch  825/3125 train_loss = 0.898\n",
      "2018-12-06T15:44:01.014668: Epoch   3 Batch  845/3125 train_loss = 0.889\n",
      "2018-12-06T15:44:01.267652: Epoch   3 Batch  865/3125 train_loss = 1.009\n",
      "2018-12-06T15:44:01.521481: Epoch   3 Batch  885/3125 train_loss = 0.992\n",
      "2018-12-06T15:44:01.778301: Epoch   3 Batch  905/3125 train_loss = 1.008\n",
      "2018-12-06T15:44:02.035612: Epoch   3 Batch  925/3125 train_loss = 0.901\n",
      "2018-12-06T15:44:02.287937: Epoch   3 Batch  945/3125 train_loss = 0.980\n",
      "2018-12-06T15:44:02.540260: Epoch   3 Batch  965/3125 train_loss = 0.835\n",
      "2018-12-06T15:44:02.792585: Epoch   3 Batch  985/3125 train_loss = 1.065\n",
      "2018-12-06T15:44:03.045803: Epoch   3 Batch 1005/3125 train_loss = 0.779\n",
      "2018-12-06T15:44:03.302618: Epoch   3 Batch 1025/3125 train_loss = 0.905\n",
      "2018-12-06T15:44:03.555941: Epoch   3 Batch 1045/3125 train_loss = 1.130\n",
      "2018-12-06T15:44:03.809261: Epoch   3 Batch 1065/3125 train_loss = 0.940\n",
      "2018-12-06T15:44:04.056599: Epoch   3 Batch 1085/3125 train_loss = 0.830\n",
      "2018-12-06T15:44:04.313910: Epoch   3 Batch 1105/3125 train_loss = 0.802\n",
      "2018-12-06T15:44:04.563784: Epoch   3 Batch 1125/3125 train_loss = 0.893\n",
      "2018-12-06T15:44:04.814620: Epoch   3 Batch 1145/3125 train_loss = 0.989\n",
      "2018-12-06T15:44:05.065947: Epoch   3 Batch 1165/3125 train_loss = 1.027\n",
      "2018-12-06T15:44:05.321264: Epoch   3 Batch 1185/3125 train_loss = 0.820\n",
      "2018-12-06T15:44:05.575091: Epoch   3 Batch 1205/3125 train_loss = 0.875\n",
      "2018-12-06T15:44:05.826417: Epoch   3 Batch 1225/3125 train_loss = 0.922\n",
      "2018-12-06T15:44:06.078754: Epoch   3 Batch 1245/3125 train_loss = 1.028\n",
      "2018-12-06T15:44:06.334070: Epoch   3 Batch 1265/3125 train_loss = 0.964\n",
      "2018-12-06T15:44:06.591381: Epoch   3 Batch 1285/3125 train_loss = 1.024\n",
      "2018-12-06T15:44:06.842708: Epoch   3 Batch 1305/3125 train_loss = 0.808\n",
      "2018-12-06T15:44:07.095538: Epoch   3 Batch 1325/3125 train_loss = 0.885\n",
      "2018-12-06T15:44:07.353845: Epoch   3 Batch 1345/3125 train_loss = 0.979\n",
      "2018-12-06T15:44:07.605173: Epoch   3 Batch 1365/3125 train_loss = 0.890\n",
      "2018-12-06T15:44:07.858495: Epoch   3 Batch 1385/3125 train_loss = 0.826\n",
      "2018-12-06T15:44:08.110328: Epoch   3 Batch 1405/3125 train_loss = 0.887\n",
      "2018-12-06T15:44:08.366642: Epoch   3 Batch 1425/3125 train_loss = 1.139\n",
      "2018-12-06T15:44:08.616971: Epoch   3 Batch 1445/3125 train_loss = 1.014\n",
      "2018-12-06T15:44:08.865306: Epoch   3 Batch 1465/3125 train_loss = 0.945\n",
      "2018-12-06T15:44:09.117631: Epoch   3 Batch 1485/3125 train_loss = 1.011\n",
      "2018-12-06T15:44:09.367960: Epoch   3 Batch 1505/3125 train_loss = 0.795\n",
      "2018-12-06T15:44:09.616263: Epoch   3 Batch 1525/3125 train_loss = 0.816\n",
      "2018-12-06T15:44:09.872217: Epoch   3 Batch 1545/3125 train_loss = 0.979\n",
      "2018-12-06T15:44:10.129935: Epoch   3 Batch 1565/3125 train_loss = 0.953\n",
      "2018-12-06T15:44:10.389241: Epoch   3 Batch 1585/3125 train_loss = 0.834\n",
      "2018-12-06T15:44:10.642563: Epoch   3 Batch 1605/3125 train_loss = 0.865\n",
      "2018-12-06T15:44:10.893890: Epoch   3 Batch 1625/3125 train_loss = 1.005\n",
      "2018-12-06T15:44:11.147211: Epoch   3 Batch 1645/3125 train_loss = 0.993\n",
      "2018-12-06T15:44:11.401530: Epoch   3 Batch 1665/3125 train_loss = 0.919\n",
      "2018-12-06T15:44:11.653869: Epoch   3 Batch 1685/3125 train_loss = 1.088\n",
      "2018-12-06T15:44:11.906193: Epoch   3 Batch 1705/3125 train_loss = 0.940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-06T15:44:12.166496: Epoch   3 Batch 1725/3125 train_loss = 0.876\n",
      "2018-12-06T15:44:12.420982: Epoch   3 Batch 1745/3125 train_loss = 0.863\n",
      "2018-12-06T15:44:12.677296: Epoch   3 Batch 1765/3125 train_loss = 0.877\n",
      "2018-12-06T15:44:12.927422: Epoch   3 Batch 1785/3125 train_loss = 1.031\n",
      "2018-12-06T15:44:13.178762: Epoch   3 Batch 1805/3125 train_loss = 1.015\n",
      "2018-12-06T15:44:13.427096: Epoch   3 Batch 1825/3125 train_loss = 1.065\n",
      "2018-12-06T15:44:13.678423: Epoch   3 Batch 1845/3125 train_loss = 1.005\n",
      "2018-12-06T15:44:13.925761: Epoch   3 Batch 1865/3125 train_loss = 0.867\n",
      "2018-12-06T15:44:14.177087: Epoch   3 Batch 1885/3125 train_loss = 0.955\n",
      "2018-12-06T15:44:14.426420: Epoch   3 Batch 1905/3125 train_loss = 0.881\n",
      "2018-12-06T15:44:14.677746: Epoch   3 Batch 1925/3125 train_loss = 0.870\n",
      "2018-12-06T15:44:14.935058: Epoch   3 Batch 1945/3125 train_loss = 0.948\n",
      "2018-12-06T15:44:15.184390: Epoch   3 Batch 1965/3125 train_loss = 0.832\n",
      "2018-12-06T15:44:15.437106: Epoch   3 Batch 1985/3125 train_loss = 0.909\n",
      "2018-12-06T15:44:15.687940: Epoch   3 Batch 2005/3125 train_loss = 0.934\n",
      "2018-12-06T15:44:15.936276: Epoch   3 Batch 2025/3125 train_loss = 0.939\n",
      "2018-12-06T15:44:16.190595: Epoch   3 Batch 2045/3125 train_loss = 0.819\n",
      "2018-12-06T15:44:16.440433: Epoch   3 Batch 2065/3125 train_loss = 0.786\n",
      "2018-12-06T15:44:16.684778: Epoch   3 Batch 2085/3125 train_loss = 0.977\n",
      "2018-12-06T15:44:16.932116: Epoch   3 Batch 2105/3125 train_loss = 0.875\n",
      "2018-12-06T15:44:17.183072: Epoch   3 Batch 2125/3125 train_loss = 1.003\n",
      "2018-12-06T15:44:17.428415: Epoch   3 Batch 2145/3125 train_loss = 1.026\n",
      "2018-12-06T15:44:17.677747: Epoch   3 Batch 2165/3125 train_loss = 0.818\n",
      "2018-12-06T15:44:17.925085: Epoch   3 Batch 2185/3125 train_loss = 0.997\n",
      "2018-12-06T15:44:18.178915: Epoch   3 Batch 2205/3125 train_loss = 0.972\n",
      "2018-12-06T15:44:18.429244: Epoch   3 Batch 2225/3125 train_loss = 0.894\n",
      "2018-12-06T15:44:18.676090: Epoch   3 Batch 2245/3125 train_loss = 0.828\n",
      "2018-12-06T15:44:18.931407: Epoch   3 Batch 2265/3125 train_loss = 0.951\n",
      "2018-12-06T15:44:19.189715: Epoch   3 Batch 2285/3125 train_loss = 1.058\n",
      "2018-12-06T15:44:19.439554: Epoch   3 Batch 2305/3125 train_loss = 0.844\n",
      "2018-12-06T15:44:19.688886: Epoch   3 Batch 2325/3125 train_loss = 0.834\n",
      "2018-12-06T15:44:19.940008: Epoch   3 Batch 2345/3125 train_loss = 0.903\n",
      "2018-12-06T15:44:20.192332: Epoch   3 Batch 2365/3125 train_loss = 0.813\n",
      "2018-12-06T15:44:20.443659: Epoch   3 Batch 2385/3125 train_loss = 0.967\n",
      "2018-12-06T15:44:20.691995: Epoch   3 Batch 2405/3125 train_loss = 0.943\n",
      "2018-12-06T15:44:20.942338: Epoch   3 Batch 2425/3125 train_loss = 0.902\n",
      "2018-12-06T15:44:21.192667: Epoch   3 Batch 2445/3125 train_loss = 1.002\n",
      "2018-12-06T15:44:21.442632: Epoch   3 Batch 2465/3125 train_loss = 0.844\n",
      "2018-12-06T15:44:21.687485: Epoch   3 Batch 2485/3125 train_loss = 0.869\n",
      "2018-12-06T15:44:21.932828: Epoch   3 Batch 2505/3125 train_loss = 0.916\n",
      "2018-12-06T15:44:22.188277: Epoch   3 Batch 2525/3125 train_loss = 0.851\n",
      "2018-12-06T15:44:22.447583: Epoch   3 Batch 2545/3125 train_loss = 0.994\n",
      "2018-12-06T15:44:22.697278: Epoch   3 Batch 2565/3125 train_loss = 0.908\n",
      "2018-12-06T15:44:22.948605: Epoch   3 Batch 2585/3125 train_loss = 0.818\n",
      "2018-12-06T15:44:23.199932: Epoch   3 Batch 2605/3125 train_loss = 0.874\n",
      "2018-12-06T15:44:23.451259: Epoch   3 Batch 2625/3125 train_loss = 1.057\n",
      "2018-12-06T15:44:23.704088: Epoch   3 Batch 2645/3125 train_loss = 0.924\n",
      "2018-12-06T15:44:23.957409: Epoch   3 Batch 2665/3125 train_loss = 0.970\n",
      "2018-12-06T15:44:24.207364: Epoch   3 Batch 2685/3125 train_loss = 0.894\n",
      "2018-12-06T15:44:24.456697: Epoch   3 Batch 2705/3125 train_loss = 0.824\n",
      "2018-12-06T15:44:24.705032: Epoch   3 Batch 2725/3125 train_loss = 1.042\n",
      "2018-12-06T15:44:24.955363: Epoch   3 Batch 2745/3125 train_loss = 0.935\n",
      "2018-12-06T15:44:25.207686: Epoch   3 Batch 2765/3125 train_loss = 0.853\n",
      "2018-12-06T15:44:25.455025: Epoch   3 Batch 2785/3125 train_loss = 0.990\n",
      "2018-12-06T15:44:25.707233: Epoch   3 Batch 2805/3125 train_loss = 0.863\n",
      "2018-12-06T15:44:25.959557: Epoch   3 Batch 2825/3125 train_loss = 0.929\n",
      "2018-12-06T15:44:26.214873: Epoch   3 Batch 2845/3125 train_loss = 0.935\n",
      "2018-12-06T15:44:26.467198: Epoch   3 Batch 2865/3125 train_loss = 0.856\n",
      "2018-12-06T15:44:26.719523: Epoch   3 Batch 2885/3125 train_loss = 0.882\n",
      "2018-12-06T15:44:26.973263: Epoch   3 Batch 2905/3125 train_loss = 1.004\n",
      "2018-12-06T15:44:27.239058: Epoch   3 Batch 2925/3125 train_loss = 0.971\n",
      "2018-12-06T15:44:27.491381: Epoch   3 Batch 2945/3125 train_loss = 0.945\n",
      "2018-12-06T15:44:27.741711: Epoch   3 Batch 2965/3125 train_loss = 0.988\n",
      "2018-12-06T15:44:27.985061: Epoch   3 Batch 2985/3125 train_loss = 0.821\n",
      "2018-12-06T15:44:28.239137: Epoch   3 Batch 3005/3125 train_loss = 0.857\n",
      "2018-12-06T15:44:28.491068: Epoch   3 Batch 3025/3125 train_loss = 0.954\n",
      "2018-12-06T15:44:28.745893: Epoch   3 Batch 3045/3125 train_loss = 0.891\n",
      "2018-12-06T15:44:28.998218: Epoch   3 Batch 3065/3125 train_loss = 0.904\n",
      "2018-12-06T15:44:29.253535: Epoch   3 Batch 3085/3125 train_loss = 0.914\n",
      "2018-12-06T15:44:29.511842: Epoch   3 Batch 3105/3125 train_loss = 0.980\n",
      "2018-12-06T15:44:29.846894: Epoch   3 Batch   17/781 test_loss = 0.928\n",
      "2018-12-06T15:44:29.957105: Epoch   3 Batch   37/781 test_loss = 0.895\n",
      "2018-12-06T15:44:30.069113: Epoch   3 Batch   57/781 test_loss = 0.949\n",
      "2018-12-06T15:44:30.184312: Epoch   3 Batch   77/781 test_loss = 0.975\n",
      "2018-12-06T15:44:30.298513: Epoch   3 Batch   97/781 test_loss = 0.799\n",
      "2018-12-06T15:44:30.411212: Epoch   3 Batch  117/781 test_loss = 1.032\n",
      "2018-12-06T15:44:30.531889: Epoch   3 Batch  137/781 test_loss = 0.945\n",
      "2018-12-06T15:44:30.648576: Epoch   3 Batch  157/781 test_loss = 0.941\n",
      "2018-12-06T15:44:30.764266: Epoch   3 Batch  177/781 test_loss = 0.930\n",
      "2018-12-06T15:44:30.881951: Epoch   3 Batch  197/781 test_loss = 0.901\n",
      "2018-12-06T15:44:30.993652: Epoch   3 Batch  217/781 test_loss = 0.840\n",
      "2018-12-06T15:44:31.105353: Epoch   3 Batch  237/781 test_loss = 0.809\n",
      "2018-12-06T15:44:31.222016: Epoch   3 Batch  257/781 test_loss = 1.055\n",
      "2018-12-06T15:44:31.336709: Epoch   3 Batch  277/781 test_loss = 1.001\n",
      "2018-12-06T15:44:31.450404: Epoch   3 Batch  297/781 test_loss = 0.981\n",
      "2018-12-06T15:44:31.562105: Epoch   3 Batch  317/781 test_loss = 1.030\n",
      "2018-12-06T15:44:31.674139: Epoch   3 Batch  337/781 test_loss = 0.941\n",
      "2018-12-06T15:44:31.785840: Epoch   3 Batch  357/781 test_loss = 0.957\n",
      "2018-12-06T15:44:31.896544: Epoch   3 Batch  377/781 test_loss = 0.963\n",
      "2018-12-06T15:44:32.010239: Epoch   3 Batch  397/781 test_loss = 0.927\n",
      "2018-12-06T15:44:32.122937: Epoch   3 Batch  417/781 test_loss = 0.875\n",
      "2018-12-06T15:44:32.240622: Epoch   3 Batch  437/781 test_loss = 0.842\n",
      "2018-12-06T15:44:32.353320: Epoch   3 Batch  457/781 test_loss = 0.789\n",
      "2018-12-06T15:44:32.461032: Epoch   3 Batch  477/781 test_loss = 0.966\n",
      "2018-12-06T15:44:32.573730: Epoch   3 Batch  497/781 test_loss = 0.843\n",
      "2018-12-06T15:44:32.686428: Epoch   3 Batch  517/781 test_loss = 0.826\n",
      "2018-12-06T15:44:32.798129: Epoch   3 Batch  537/781 test_loss = 0.851\n",
      "2018-12-06T15:44:32.909830: Epoch   3 Batch  557/781 test_loss = 1.056\n",
      "2018-12-06T15:44:33.023530: Epoch   3 Batch  577/781 test_loss = 0.921\n",
      "2018-12-06T15:44:33.136228: Epoch   3 Batch  597/781 test_loss = 0.889\n",
      "2018-12-06T15:44:33.250429: Epoch   3 Batch  617/781 test_loss = 0.868\n",
      "2018-12-06T15:44:33.362635: Epoch   3 Batch  637/781 test_loss = 0.875\n",
      "2018-12-06T15:44:33.476331: Epoch   3 Batch  657/781 test_loss = 1.031\n",
      "2018-12-06T15:44:33.589029: Epoch   3 Batch  677/781 test_loss = 1.015\n",
      "2018-12-06T15:44:33.703722: Epoch   3 Batch  697/781 test_loss = 0.942\n",
      "2018-12-06T15:44:33.814425: Epoch   3 Batch  717/781 test_loss = 0.930\n",
      "2018-12-06T15:44:33.924132: Epoch   3 Batch  737/781 test_loss = 0.840\n",
      "2018-12-06T15:44:34.038824: Epoch   3 Batch  757/781 test_loss = 1.082\n",
      "2018-12-06T15:44:34.160498: Epoch   3 Batch  777/781 test_loss = 0.931\n",
      "2018-12-06T15:44:34.640127: Epoch   4 Batch    0/3125 train_loss = 1.002\n",
      "2018-12-06T15:44:34.894947: Epoch   4 Batch   20/3125 train_loss = 0.918\n",
      "2018-12-06T15:44:35.147272: Epoch   4 Batch   40/3125 train_loss = 0.933\n",
      "2018-12-06T15:44:35.399597: Epoch   4 Batch   60/3125 train_loss = 0.801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-06T15:44:35.651921: Epoch   4 Batch   80/3125 train_loss = 0.909\n",
      "2018-12-06T15:44:35.899259: Epoch   4 Batch  100/3125 train_loss = 0.970\n",
      "2018-12-06T15:44:36.147594: Epoch   4 Batch  120/3125 train_loss = 0.988\n",
      "2018-12-06T15:44:36.398430: Epoch   4 Batch  140/3125 train_loss = 0.941\n",
      "2018-12-06T15:44:36.644772: Epoch   4 Batch  160/3125 train_loss = 0.829\n",
      "2018-12-06T15:44:36.893106: Epoch   4 Batch  180/3125 train_loss = 0.887\n",
      "2018-12-06T15:44:37.140444: Epoch   4 Batch  200/3125 train_loss = 1.107\n",
      "2018-12-06T15:44:37.391761: Epoch   4 Batch  220/3125 train_loss = 0.928\n",
      "2018-12-06T15:44:37.638611: Epoch   4 Batch  240/3125 train_loss = 0.990\n",
      "2018-12-06T15:44:37.885949: Epoch   4 Batch  260/3125 train_loss = 0.997\n",
      "2018-12-06T15:44:38.135282: Epoch   4 Batch  280/3125 train_loss = 0.974\n",
      "2018-12-06T15:44:38.388603: Epoch   4 Batch  300/3125 train_loss = 1.094\n",
      "2018-12-06T15:44:38.636941: Epoch   4 Batch  320/3125 train_loss = 1.007\n",
      "2018-12-06T15:44:38.882284: Epoch   4 Batch  340/3125 train_loss = 0.783\n",
      "2018-12-06T15:44:39.127517: Epoch   4 Batch  360/3125 train_loss = 0.838\n",
      "2018-12-06T15:44:39.384828: Epoch   4 Batch  380/3125 train_loss = 0.919\n",
      "2018-12-06T15:44:39.630171: Epoch   4 Batch  400/3125 train_loss = 0.870\n",
      "2018-12-06T15:44:39.877508: Epoch   4 Batch  420/3125 train_loss = 0.833\n",
      "2018-12-06T15:44:40.126248: Epoch   4 Batch  440/3125 train_loss = 0.918\n",
      "2018-12-06T15:44:40.382483: Epoch   4 Batch  460/3125 train_loss = 0.924\n",
      "2018-12-06T15:44:40.631816: Epoch   4 Batch  480/3125 train_loss = 1.011\n",
      "2018-12-06T15:44:40.889126: Epoch   4 Batch  500/3125 train_loss = 0.743\n",
      "2018-12-06T15:44:41.146437: Epoch   4 Batch  520/3125 train_loss = 0.974\n",
      "2018-12-06T15:44:41.404253: Epoch   4 Batch  540/3125 train_loss = 0.871\n",
      "2018-12-06T15:44:41.651099: Epoch   4 Batch  560/3125 train_loss = 1.046\n",
      "2018-12-06T15:44:41.921473: Epoch   4 Batch  580/3125 train_loss = 1.029\n",
      "2018-12-06T15:44:42.179781: Epoch   4 Batch  600/3125 train_loss = 0.895\n",
      "2018-12-06T15:44:42.437092: Epoch   4 Batch  620/3125 train_loss = 0.948\n",
      "2018-12-06T15:44:42.691411: Epoch   4 Batch  640/3125 train_loss = 0.903\n",
      "2018-12-06T15:44:42.937757: Epoch   4 Batch  660/3125 train_loss = 0.885\n",
      "2018-12-06T15:44:43.214252: Epoch   4 Batch  680/3125 train_loss = 0.963\n",
      "2018-12-06T15:44:43.512471: Epoch   4 Batch  700/3125 train_loss = 0.924\n",
      "2018-12-06T15:44:43.784742: Epoch   4 Batch  720/3125 train_loss = 0.842\n",
      "2018-12-06T15:44:44.047544: Epoch   4 Batch  740/3125 train_loss = 0.943\n",
      "2018-12-06T15:44:44.311839: Epoch   4 Batch  760/3125 train_loss = 0.835\n",
      "2018-12-06T15:44:44.560386: Epoch   4 Batch  780/3125 train_loss = 0.939\n",
      "2018-12-06T15:44:44.804240: Epoch   4 Batch  800/3125 train_loss = 0.827\n",
      "2018-12-06T15:44:45.051578: Epoch   4 Batch  820/3125 train_loss = 0.898\n",
      "2018-12-06T15:44:45.304899: Epoch   4 Batch  840/3125 train_loss = 0.903\n",
      "2018-12-06T15:44:45.556229: Epoch   4 Batch  860/3125 train_loss = 0.840\n",
      "2018-12-06T15:44:45.806558: Epoch   4 Batch  880/3125 train_loss = 0.849\n",
      "2018-12-06T15:44:46.056396: Epoch   4 Batch  900/3125 train_loss = 0.899\n",
      "2018-12-06T15:44:46.305743: Epoch   4 Batch  920/3125 train_loss = 0.879\n",
      "2018-12-06T15:44:46.562056: Epoch   4 Batch  940/3125 train_loss = 0.925\n",
      "2018-12-06T15:44:46.813383: Epoch   4 Batch  960/3125 train_loss = 0.900\n",
      "2018-12-06T15:44:47.064217: Epoch   4 Batch  980/3125 train_loss = 1.036\n",
      "2018-12-06T15:44:47.318536: Epoch   4 Batch 1000/3125 train_loss = 0.983\n",
      "2018-12-06T15:44:47.584085: Epoch   4 Batch 1020/3125 train_loss = 0.951\n",
      "2018-12-06T15:44:47.848393: Epoch   4 Batch 1040/3125 train_loss = 0.750\n",
      "2018-12-06T15:44:48.110201: Epoch   4 Batch 1060/3125 train_loss = 0.986\n",
      "2018-12-06T15:44:48.376488: Epoch   4 Batch 1080/3125 train_loss = 0.860\n",
      "2018-12-06T15:44:48.658732: Epoch   4 Batch 1100/3125 train_loss = 0.963\n",
      "2018-12-06T15:44:48.911056: Epoch   4 Batch 1120/3125 train_loss = 0.925\n",
      "2018-12-06T15:44:49.166293: Epoch   4 Batch 1140/3125 train_loss = 0.906\n",
      "2018-12-06T15:44:49.428603: Epoch   4 Batch 1160/3125 train_loss = 0.839\n",
      "2018-12-06T15:44:49.679264: Epoch   4 Batch 1180/3125 train_loss = 0.872\n",
      "2018-12-06T15:44:49.929593: Epoch   4 Batch 1200/3125 train_loss = 0.948\n",
      "2018-12-06T15:44:50.180920: Epoch   4 Batch 1220/3125 train_loss = 0.913\n",
      "2018-12-06T15:44:50.432247: Epoch   4 Batch 1240/3125 train_loss = 0.777\n",
      "2018-12-06T15:44:50.682577: Epoch   4 Batch 1260/3125 train_loss = 0.911\n",
      "2018-12-06T15:44:50.933906: Epoch   4 Batch 1280/3125 train_loss = 0.975\n",
      "2018-12-06T15:44:51.187228: Epoch   4 Batch 1300/3125 train_loss = 0.829\n",
      "2018-12-06T15:44:51.440550: Epoch   4 Batch 1320/3125 train_loss = 0.865\n",
      "2018-12-06T15:44:51.689389: Epoch   4 Batch 1340/3125 train_loss = 0.833\n",
      "2018-12-06T15:44:51.940717: Epoch   4 Batch 1360/3125 train_loss = 0.852\n",
      "2018-12-06T15:44:52.194038: Epoch   4 Batch 1380/3125 train_loss = 0.835\n",
      "2018-12-06T15:44:52.451363: Epoch   4 Batch 1400/3125 train_loss = 0.918\n",
      "2018-12-06T15:44:52.702703: Epoch   4 Batch 1420/3125 train_loss = 0.890\n",
      "2018-12-06T15:44:52.953033: Epoch   4 Batch 1440/3125 train_loss = 0.774\n",
      "2018-12-06T15:44:53.206355: Epoch   4 Batch 1460/3125 train_loss = 0.896\n",
      "2018-12-06T15:44:53.458683: Epoch   4 Batch 1480/3125 train_loss = 0.896\n",
      "2018-12-06T15:44:53.718063: Epoch   4 Batch 1500/3125 train_loss = 0.965\n",
      "2018-12-06T15:44:53.973211: Epoch   4 Batch 1520/3125 train_loss = 0.847\n",
      "2018-12-06T15:44:54.226043: Epoch   4 Batch 1540/3125 train_loss = 0.973\n",
      "2018-12-06T15:44:54.480361: Epoch   4 Batch 1560/3125 train_loss = 0.848\n",
      "2018-12-06T15:44:54.736675: Epoch   4 Batch 1580/3125 train_loss = 0.949\n",
      "2018-12-06T15:44:54.988005: Epoch   4 Batch 1600/3125 train_loss = 0.841\n",
      "2018-12-06T15:44:55.241326: Epoch   4 Batch 1620/3125 train_loss = 0.832\n",
      "2018-12-06T15:44:55.493799: Epoch   4 Batch 1640/3125 train_loss = 0.993\n",
      "2018-12-06T15:44:55.745632: Epoch   4 Batch 1660/3125 train_loss = 0.980\n",
      "2018-12-06T15:44:56.008927: Epoch   4 Batch 1680/3125 train_loss = 0.863\n",
      "2018-12-06T15:44:56.268232: Epoch   4 Batch 1700/3125 train_loss = 0.787\n",
      "2018-12-06T15:44:56.518457: Epoch   4 Batch 1720/3125 train_loss = 0.959\n",
      "2018-12-06T15:44:56.770781: Epoch   4 Batch 1740/3125 train_loss = 0.941\n",
      "2018-12-06T15:44:57.020117: Epoch   4 Batch 1760/3125 train_loss = 0.934\n",
      "2018-12-06T15:44:57.270447: Epoch   4 Batch 1780/3125 train_loss = 0.929\n",
      "2018-12-06T15:44:57.522771: Epoch   4 Batch 1800/3125 train_loss = 0.857\n",
      "2018-12-06T15:44:57.773100: Epoch   4 Batch 1820/3125 train_loss = 0.828\n",
      "2018-12-06T15:44:58.027989: Epoch   4 Batch 1840/3125 train_loss = 0.965\n",
      "2018-12-06T15:44:58.283305: Epoch   4 Batch 1860/3125 train_loss = 0.963\n",
      "2018-12-06T15:44:58.533648: Epoch   4 Batch 1880/3125 train_loss = 0.932\n",
      "2018-12-06T15:44:58.783978: Epoch   4 Batch 1900/3125 train_loss = 0.757\n",
      "2018-12-06T15:44:59.037299: Epoch   4 Batch 1920/3125 train_loss = 0.875\n",
      "2018-12-06T15:44:59.288626: Epoch   4 Batch 1940/3125 train_loss = 0.798\n",
      "2018-12-06T15:44:59.542423: Epoch   4 Batch 1960/3125 train_loss = 0.817\n",
      "2018-12-06T15:44:59.791770: Epoch   4 Batch 1980/3125 train_loss = 0.843\n",
      "2018-12-06T15:45:00.047087: Epoch   4 Batch 2000/3125 train_loss = 1.081\n",
      "2018-12-06T15:45:00.313373: Epoch   4 Batch 2020/3125 train_loss = 0.961\n",
      "2018-12-06T15:45:00.569687: Epoch   4 Batch 2040/3125 train_loss = 0.825\n",
      "2018-12-06T15:45:00.825336: Epoch   4 Batch 2060/3125 train_loss = 0.840\n",
      "2018-12-06T15:45:01.075503: Epoch   4 Batch 2080/3125 train_loss = 1.011\n",
      "2018-12-06T15:45:01.326337: Epoch   4 Batch 2100/3125 train_loss = 0.833\n",
      "2018-12-06T15:45:01.578171: Epoch   4 Batch 2120/3125 train_loss = 0.854\n",
      "2018-12-06T15:45:01.834485: Epoch   4 Batch 2140/3125 train_loss = 0.870\n",
      "2018-12-06T15:45:02.089801: Epoch   4 Batch 2160/3125 train_loss = 0.865\n",
      "2018-12-06T15:45:02.351101: Epoch   4 Batch 2180/3125 train_loss = 0.936\n",
      "2018-12-06T15:45:02.610137: Epoch   4 Batch 2200/3125 train_loss = 0.843\n",
      "2018-12-06T15:45:02.867971: Epoch   4 Batch 2220/3125 train_loss = 0.880\n",
      "2018-12-06T15:45:03.121291: Epoch   4 Batch 2240/3125 train_loss = 0.860\n",
      "2018-12-06T15:45:03.376608: Epoch   4 Batch 2260/3125 train_loss = 0.957\n",
      "2018-12-06T15:45:03.632922: Epoch   4 Batch 2280/3125 train_loss = 0.897\n",
      "2018-12-06T15:45:03.889238: Epoch   4 Batch 2300/3125 train_loss = 0.877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-06T15:45:04.141528: Epoch   4 Batch 2320/3125 train_loss = 0.918\n",
      "2018-12-06T15:45:04.403841: Epoch   4 Batch 2340/3125 train_loss = 0.881\n",
      "2018-12-06T15:45:04.652176: Epoch   4 Batch 2360/3125 train_loss = 0.900\n",
      "2018-12-06T15:45:04.901508: Epoch   4 Batch 2380/3125 train_loss = 0.906\n",
      "2018-12-06T15:45:05.147849: Epoch   4 Batch 2400/3125 train_loss = 0.957\n",
      "2018-12-06T15:45:05.402195: Epoch   4 Batch 2420/3125 train_loss = 0.783\n",
      "2018-12-06T15:45:05.649715: Epoch   4 Batch 2440/3125 train_loss = 0.850\n",
      "2018-12-06T15:45:05.900044: Epoch   4 Batch 2460/3125 train_loss = 0.878\n",
      "2018-12-06T15:45:06.152369: Epoch   4 Batch 2480/3125 train_loss = 1.008\n",
      "2018-12-06T15:45:06.400704: Epoch   4 Batch 2500/3125 train_loss = 0.886\n",
      "2018-12-06T15:45:06.647044: Epoch   4 Batch 2520/3125 train_loss = 0.906\n",
      "2018-12-06T15:45:06.892387: Epoch   4 Batch 2540/3125 train_loss = 0.894\n",
      "2018-12-06T15:45:07.147703: Epoch   4 Batch 2560/3125 train_loss = 0.704\n",
      "2018-12-06T15:45:07.410001: Epoch   4 Batch 2580/3125 train_loss = 0.933\n",
      "2018-12-06T15:45:07.666314: Epoch   4 Batch 2600/3125 train_loss = 0.872\n",
      "2018-12-06T15:45:07.912655: Epoch   4 Batch 2620/3125 train_loss = 0.810\n",
      "2018-12-06T15:45:08.164980: Epoch   4 Batch 2640/3125 train_loss = 0.896\n",
      "2018-12-06T15:45:08.420310: Epoch   4 Batch 2660/3125 train_loss = 0.987\n",
      "2018-12-06T15:45:08.679615: Epoch   4 Batch 2680/3125 train_loss = 0.829\n",
      "2018-12-06T15:45:08.934932: Epoch   4 Batch 2700/3125 train_loss = 0.932\n",
      "2018-12-06T15:45:09.191245: Epoch   4 Batch 2720/3125 train_loss = 0.810\n",
      "2018-12-06T15:45:09.439185: Epoch   4 Batch 2740/3125 train_loss = 0.922\n",
      "2018-12-06T15:45:09.694501: Epoch   4 Batch 2760/3125 train_loss = 0.797\n",
      "2018-12-06T15:45:09.953806: Epoch   4 Batch 2780/3125 train_loss = 0.835\n",
      "2018-12-06T15:45:10.260984: Epoch   4 Batch 2800/3125 train_loss = 1.101\n",
      "2018-12-06T15:45:10.523282: Epoch   4 Batch 2820/3125 train_loss = 1.091\n",
      "2018-12-06T15:45:10.769880: Epoch   4 Batch 2840/3125 train_loss = 0.815\n",
      "2018-12-06T15:45:11.031688: Epoch   4 Batch 2860/3125 train_loss = 0.829\n",
      "2018-12-06T15:45:11.292988: Epoch   4 Batch 2880/3125 train_loss = 0.869\n",
      "2018-12-06T15:45:11.563264: Epoch   4 Batch 2900/3125 train_loss = 0.817\n",
      "2018-12-06T15:45:11.828556: Epoch   4 Batch 2920/3125 train_loss = 0.872\n",
      "2018-12-06T15:45:12.083873: Epoch   4 Batch 2940/3125 train_loss = 0.914\n",
      "2018-12-06T15:45:12.354163: Epoch   4 Batch 2960/3125 train_loss = 0.936\n",
      "2018-12-06T15:45:12.626683: Epoch   4 Batch 2980/3125 train_loss = 0.830\n",
      "2018-12-06T15:45:12.892970: Epoch   4 Batch 3000/3125 train_loss = 0.956\n",
      "2018-12-06T15:45:13.161252: Epoch   4 Batch 3020/3125 train_loss = 1.056\n",
      "2018-12-06T15:45:13.419562: Epoch   4 Batch 3040/3125 train_loss = 0.966\n",
      "2018-12-06T15:45:13.682857: Epoch   4 Batch 3060/3125 train_loss = 0.757\n",
      "2018-12-06T15:45:13.945155: Epoch   4 Batch 3080/3125 train_loss = 1.040\n",
      "2018-12-06T15:45:14.199474: Epoch   4 Batch 3100/3125 train_loss = 1.004\n",
      "2018-12-06T15:45:14.452796: Epoch   4 Batch 3120/3125 train_loss = 0.851\n",
      "2018-12-06T15:45:14.615360: Epoch   4 Batch   16/781 test_loss = 0.886\n",
      "2018-12-06T15:45:14.732048: Epoch   4 Batch   36/781 test_loss = 0.976\n",
      "2018-12-06T15:45:14.857711: Epoch   4 Batch   56/781 test_loss = 0.966\n",
      "2018-12-06T15:45:14.981380: Epoch   4 Batch   76/781 test_loss = 0.998\n",
      "2018-12-06T15:45:15.100674: Epoch   4 Batch   96/781 test_loss = 1.032\n",
      "2018-12-06T15:45:15.219356: Epoch   4 Batch  116/781 test_loss = 0.865\n",
      "2018-12-06T15:45:15.337546: Epoch   4 Batch  136/781 test_loss = 0.816\n",
      "2018-12-06T15:45:15.451242: Epoch   4 Batch  156/781 test_loss = 0.901\n",
      "2018-12-06T15:45:15.564937: Epoch   4 Batch  176/781 test_loss = 0.921\n",
      "2018-12-06T15:45:15.680628: Epoch   4 Batch  196/781 test_loss = 0.836\n",
      "2018-12-06T15:45:15.793326: Epoch   4 Batch  216/781 test_loss = 1.005\n",
      "2018-12-06T15:45:15.908019: Epoch   4 Batch  236/781 test_loss = 0.833\n",
      "2018-12-06T15:45:16.018722: Epoch   4 Batch  256/781 test_loss = 0.846\n",
      "2018-12-06T15:45:16.131423: Epoch   4 Batch  276/781 test_loss = 1.153\n",
      "2018-12-06T15:45:16.251103: Epoch   4 Batch  296/781 test_loss = 0.854\n",
      "2018-12-06T15:45:16.365037: Epoch   4 Batch  316/781 test_loss = 0.864\n",
      "2018-12-06T15:45:16.482230: Epoch   4 Batch  336/781 test_loss = 0.755\n",
      "2018-12-06T15:45:16.596923: Epoch   4 Batch  356/781 test_loss = 0.885\n",
      "2018-12-06T15:45:16.708623: Epoch   4 Batch  376/781 test_loss = 0.938\n",
      "2018-12-06T15:45:16.821322: Epoch   4 Batch  396/781 test_loss = 0.863\n",
      "2018-12-06T15:45:16.933023: Epoch   4 Batch  416/781 test_loss = 0.987\n",
      "2018-12-06T15:45:17.047716: Epoch   4 Batch  436/781 test_loss = 0.882\n",
      "2018-12-06T15:45:17.161411: Epoch   4 Batch  456/781 test_loss = 0.793\n",
      "2018-12-06T15:45:17.272115: Epoch   4 Batch  476/781 test_loss = 0.995\n",
      "2018-12-06T15:45:17.388802: Epoch   4 Batch  496/781 test_loss = 0.958\n",
      "2018-12-06T15:45:17.503495: Epoch   4 Batch  516/781 test_loss = 0.815\n",
      "2018-12-06T15:45:17.616194: Epoch   4 Batch  536/781 test_loss = 0.935\n",
      "2018-12-06T15:45:17.731884: Epoch   4 Batch  556/781 test_loss = 0.852\n",
      "2018-12-06T15:45:17.844230: Epoch   4 Batch  576/781 test_loss = 1.017\n",
      "2018-12-06T15:45:17.958923: Epoch   4 Batch  596/781 test_loss = 0.985\n",
      "2018-12-06T15:45:18.078602: Epoch   4 Batch  616/781 test_loss = 1.008\n",
      "2018-12-06T15:45:18.206261: Epoch   4 Batch  636/781 test_loss = 0.862\n",
      "2018-12-06T15:45:18.328932: Epoch   4 Batch  656/781 test_loss = 0.912\n",
      "2018-12-06T15:45:18.447614: Epoch   4 Batch  676/781 test_loss = 1.090\n",
      "2018-12-06T15:45:18.564302: Epoch   4 Batch  696/781 test_loss = 0.801\n",
      "2018-12-06T15:45:18.677997: Epoch   4 Batch  716/781 test_loss = 0.932\n",
      "2018-12-06T15:45:18.792777: Epoch   4 Batch  736/781 test_loss = 1.124\n",
      "2018-12-06T15:45:18.905475: Epoch   4 Batch  756/781 test_loss = 0.844\n",
      "2018-12-06T15:45:19.018173: Epoch   4 Batch  776/781 test_loss = 0.801\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "## 训练网络\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "losses = {'train':[], 'test':[]}\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    # 搜集数据给tensorBoard用\n",
    "    # Keep track of gradient values and sparsity\n",
    "    grad_summaries = []\n",
    "    for g, v in gradients:\n",
    "        if g is not None:\n",
    "            grad_hist_summary = tf.summary.histogram('{}/grad/hist'.format(v.name.replace(':', '_')), g)\n",
    "            sparsity_summary = tf.summary.scalar('{}/grad/sparsity'.format(v.name.replace(':', '_')), tf.nn.zero_fraction(g))\n",
    "            grad_summaries.append(grad_hist_summary)\n",
    "            grad_summaries.append(sparsity_summary)\n",
    "    grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "    \n",
    "    # Ooutput directory for models and summaries\n",
    "    timestamp = str(int(time.time()))\n",
    "    out_dir = os.path.abspath(os.path.join(os.path.curdir, 'runs', timestamp))\n",
    "    print('Writing to {}\\n'.format(out_dir))\n",
    "    \n",
    "    # Summaries for loss and accuracy\n",
    "    loss_summary = tf.summary.scalar('loss', loss)\n",
    "    \n",
    "    # Train Summaries\n",
    "    train_summary_op = tf.summary.merge([loss_summary, grad_summaries_merged])\n",
    "    train_summary_dir = os.path.join(out_dir, 'summaries', 'train')\n",
    "    train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "    \n",
    "    # Inference summaries\n",
    "    inference_summary_op = tf.summary.merge([loss_summary])\n",
    "    inference_summary_dir = os.path.join(out_dir, 'summaries', 'inference')\n",
    "    inference_summary_writer = tf.summary.FileWriter(inference_summary_dir, sess.graph)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    for epoch_i in range(num_epochs):\n",
    "        # 将数据集分成训练集和测试机，随机种子不固定\n",
    "        train_X, test_X, train_y, test_y = train_test_split(features, targets_values, test_size=0.2, random_state=0)\n",
    "        \n",
    "        train_batches = get_batches(train_X, train_y, batch_size)\n",
    "        test_batches = get_batches(test_X, test_y, batch_size)\n",
    "        \n",
    "        # 训练的迭代，保存训练损失\n",
    "        for batch_i in range(len(train_X) // batch_size):\n",
    "            x, y = next(train_batches)\n",
    "            categories = np.zeros([batch_size, 18])\n",
    "            for i in range(batch_size):\n",
    "                categories[i] = x.take(6, 1)[i]\n",
    "            titles = np.zeros([batch_size, sentences_size])\n",
    "            for i in range(batch_size):\n",
    "                titles[i] = x.take(5, 1)[i]\n",
    "            \n",
    "            feed = {uid: np.reshape(x.take(0, 1), [batch_size, 1]),\n",
    "                    movie_id: np.reshape(x.take(1, 1), [batch_size, 1]),\n",
    "                    movie_categories: categories,\n",
    "                    movie_titles: titles,\n",
    "                    targets: np.reshape(y, [batch_size, 1]),\n",
    "                    dropout_keep_prob: dropout_keep,\n",
    "                    lr: learning_rate}\n",
    "            \n",
    "            step, train_loss, summaries, _ = sess.run([global_step, loss, train_summary_op, train_op], feed)\n",
    "            losses['train'].append(train_loss)\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "            \n",
    "            # Show every (show_every_n_batches) batches\n",
    "            if (epoch_i * (len(train_X) // batch_size) + batch_i) % show_every_n_batches == 0:\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print('{}: Epoch {:>3} Batch {:>4}/{} train_loss = {:.3f}'.format(\n",
    "                    time_str,\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    (len(train_X) // batch_size),\n",
    "                    train_loss\n",
    "                ))\n",
    "                \n",
    "        # 使用测试数据的迭代\n",
    "        for batch_i in range(len(test_X) // batch_size):\n",
    "            x, y = next(test_batches)\n",
    "            categories = np.zeros([batch_size, 18])\n",
    "            for i in range(batch_size):\n",
    "                categories[i] = x.take(6, 1)[i]\n",
    "            titles = np.zeros([batch_size, sentences_size])\n",
    "            for i in range(batch_size):\n",
    "                titles[i] = x.take(5, 1)[i]\n",
    "            \n",
    "            feed = {uid: np.reshape(x.take(0, 1), [batch_size, 1]),\n",
    "                    movie_id: np.reshape(x.take(1, 1), [batch_size, 1]),\n",
    "                    movie_categories: categories,\n",
    "                    movie_titles: titles,\n",
    "                    targets: np.reshape(y, [batch_size, 1]),\n",
    "                    dropout_keep_prob: dropout_keep,\n",
    "                    lr: learning_rate}\n",
    "            \n",
    "            step, test_loss, summaries= sess.run([global_step, loss, inference_summary_op], feed)\n",
    "            losses['test'].append(test_loss)\n",
    "            inference_summary_writer.add_summary(summaries, step)\n",
    "            \n",
    "            # Show every (show_every_n_batches) batches\n",
    "            if (epoch_i * (len(test_X) // batch_size) + batch_i) % show_every_n_batches == 0:\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print('{}: Epoch {:>3} Batch {:>4}/{} test_loss = {:.3f}'.format(\n",
    "                    time_str,\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    (len(test_X) // batch_size),\n",
    "                    test_loss\n",
    "                ))\n",
    "    # Save Model\n",
    "    saver.save(sess, save_dir)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在TensorBoard中查看可视化结果\n",
    "tensorboard --logdir /PATH_TO_CODE/runs/timestamp/summaries\n",
    "## 保存参数\n",
    "保存save_dir在生成预测时使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_params((save_dir))\n",
    "\n",
    "load_dir = load_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 显示训练Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 377
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses['train'], label='Training loss')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 显示测试Loss\n",
    "迭代次数再增加一些，下降的趋势会明显一些"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 377
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses['test'], label='Test loss')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取Tensor\n",
    "使用函数`get_tensor_by_name()`从`loaded_graph`中获取tensors，后面的推荐功能要用到。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensors(loaded_graph):\n",
    "    uid = loaded_graph.get_tensor_by_name('uid:0')\n",
    "    movie_id = loaded_graph.get_tensor_by_name('movie_id:0')\n",
    "    movie_categories = loaded_graph.get_tensor_by_name('movie_categories:0')\n",
    "    movie_titles = loaded_graph.get_tensor_by_name('movie_titles:0')\n",
    "    targets = loaded_graph.get_tensor_by_name('targets:0')\n",
    "    dropout_keep_prob = loaded_graph.get_tensor_by_name('dropout_keep_prob:0')\n",
    "    lr = loaded_graph.get_tensor_by_name('LearningRate:0')\n",
    "    inference = loaded_graph.get_tensor_by_name('inference/ExpandDims:0')\n",
    "    movie_combine_layer_flat = loaded_graph.get_tensor_by_name('movie_fc/Reshape:0')\n",
    "    user_combine_layer_flat = loaded_graph.get_tensor_by_name('user_fc/Reshape:0')\n",
    "    return uid, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob, inference, movie_combine_layer_flat, user_combine_layer_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 指定用户和电影进行评分\n",
    "这部分就是对网络做正向传播，计算得到预测的评分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rating_movie(user_id_val, movie_id_val):\n",
    "    loaded_graph = tf.Graph()\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "        loader.restore(sess, load_dir)\n",
    "        \n",
    "        # Get Tensors from loaded model\n",
    "        uid, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob, inference, _, _ = get_tensors(loaded_graph)\n",
    "        \n",
    "        categories = np.zeros([1, 18])\n",
    "        categories[0] = movies.values[movieid2idx[movie_id_val]][2]\n",
    "        \n",
    "        titles = np.zeros([1, sentences_size])\n",
    "        titles[0] = movies.values[movieid2idx[movie_id_val]][1]\n",
    "        \n",
    "        feed = {uid: np.reshape(users.values[user_id_val - 1][0], [1, 1]),\n",
    "                movie_id: np.reshape(movies.values[movieid2idx[movie_id_val]][0], [1, 1]),\n",
    "                movie_categories: categories,\n",
    "                movie_titles: titles,\n",
    "                dropout_keep_prob: 1}\n",
    "        \n",
    "        # Get prediction\n",
    "        inference_val = sess.run([inference], feed)\n",
    "        \n",
    "        return (inference_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[3.5976136]], dtype=float32)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_movie(234, 1401)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成Movie特征矩阵\n",
    "将训练好的电影特征组合成电影特征矩阵并保存到本地"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n"
     ]
    }
   ],
   "source": [
    "loaded_graph = tf.Graph()\n",
    "movie_matrics = []\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "    \n",
    "    # Get Tensors from loaded model\n",
    "    uid, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob, _, movie_combine_layer_flat, _ = get_tensors(loaded_graph)\n",
    "    \n",
    "    for item in movies.values:\n",
    "        categories = np.zeros([1, 18])\n",
    "        categories[0] = item.take(2)\n",
    "        \n",
    "        titles = np.zeros([1, sentences_size])\n",
    "        titles[0] = item.take(1)\n",
    "        \n",
    "        feed = {movie_id: np.reshape(item.take(0), [1, 1]),\n",
    "                movie_categories: categories,\n",
    "                movie_titles: titles,\n",
    "                dropout_keep_prob: 1}\n",
    "        \n",
    "        movie_combine_layer_flat_val = sess.run([movie_combine_layer_flat], feed)\n",
    "        movie_matrics.append(movie_combine_layer_flat_val)\n",
    "\n",
    "pickle.dump((np.array(movie_matrics).reshape(-1, 200)), open('movie_matrics.p', 'wb'))\n",
    "movie_matrics = pickle.load(open('movie_matrics.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_matrics = pickle.load(open('movie_matrics.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成User特征矩阵\n",
    "将训练好的用户特征组合成用户特征矩阵并保存到本地"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n"
     ]
    }
   ],
   "source": [
    "loaded_graph = tf.Graph()\n",
    "user_matrics = []\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "    \n",
    "    # Get Tensors from loaded model\n",
    "    uid, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob, _, _, user_combine_layer_flat = get_tensors(loaded_graph)\n",
    "    \n",
    "    \n",
    "    for item in users.values:\n",
    "        feed = {uid: np.reshape(item.take(0), [1, 1]),\n",
    "                dropout_keep_prob: 1}\n",
    "        \n",
    "        user_combine_layer_flat_val = sess.run([user_combine_layer_flat], feed)\n",
    "        user_matrics.append(user_combine_layer_flat_val)\n",
    "\n",
    "pickle.dump((np.array(user_matrics).reshape(-1, 200)), open('user_matrics.p', 'wb'))\n",
    "user_matrics = pickle.load(open('user_matrics.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_matrics = pickle.load(open('user_matrics.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开始推荐电影\n",
    "使用生产的用户特征矩阵和电影特征矩阵做电影推荐\n",
    "\n",
    "### 推荐同类型的电影\n",
    "思路是计算当前看的电影特征向量与整个电影特征矩阵的余弦相似度，取相似度最大的top_k个，这里加了些随机选择再里面，保证每次的推荐稍稍有些不同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_same_type_movie(movie_id_val, top_k=20):\n",
    "    loaded_graph = tf.Graph()\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load saved model\n",
    "        loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "        loader.restore(sess, load_dir)\n",
    "        \n",
    "        norm_movie_matrics = tf.sqrt(tf.reduce_sum(tf.square(movie_matrics), 1, keep_dims=True))\n",
    "        normalized_movie_matrics = movie_matrics / norm_movie_matrics\n",
    "        \n",
    "        # 推荐同类型的电影\n",
    "        probs_embeddings = (movie_matrics[movieid2idx[movie_id_val]]).reshape([1, 200])\n",
    "        probs_similarity = tf.matmul(probs_embeddings, tf.transpose(normalized_movie_matrics))\n",
    "        sim = (probs_similarity.eval())\n",
    "        \n",
    "        print('您看的电影是：{}'.format(movies_orig[movieid2idx[movie_id_val]]))\n",
    "        print('以下是给您推荐的：')\n",
    "        p = np.squeeze(sim)\n",
    "        p[np.argsort(p)[:-top_k]] = 0\n",
    "        p = p/ np.sum(p)\n",
    "        results = set()\n",
    "        while len(results) != 5:\n",
    "            c = np.random.choice(3883, 1, p=p)[0]\n",
    "            results.add(c)\n",
    "        for val in (results):\n",
    "            print(val)\n",
    "            print(movies_orig[val])\n",
    "    \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n",
      "WARNING:tensorflow:From <ipython-input-37-f868f7807a1a>:8: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "您看的电影是：[1401 'Ghosts of Mississippi (1996)' 'Drama']\n",
      "以下是给您推荐的：\n",
      "3457\n",
      "[3526 'Parenthood (1989)' 'Comedy|Drama']\n",
      "1380\n",
      "[1401 'Ghosts of Mississippi (1996)' 'Drama']\n",
      "3629\n",
      "[3698 'Running Man, The (1987)' 'Action|Adventure|Sci-Fi']\n",
      "3024\n",
      "[3093 'McCabe & Mrs. Miller (1971)' 'Drama|Western']\n",
      "954\n",
      "[966 'Walk in the Sun, A (1945)' 'Drama']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{954, 1380, 3024, 3457, 3629}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend_same_type_movie(1401, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 推荐您喜欢的电影\n",
    "思路是使用用户特征向量和电影特征矩阵计算所有电影的评分，取评分最高的top_k个，同样加入了些随机选择部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_your_favorite_movie(user_id_val, top_k=10):\n",
    "    loaded_graph = tf.Graph()\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load saved model\n",
    "        loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "        loader.restore(sess, load_dir)\n",
    "        \n",
    "        # 推荐您喜欢的电影\n",
    "        probs_embeddings = (user_matrics[user_id_val - 1]).reshape([1, 200])\n",
    "        \n",
    "        probs_similarity = tf.matmul(probs_embeddings, tf.transpose(movie_matrics))\n",
    "        sim = (probs_similarity.eval())\n",
    "        \n",
    "        print('以下是给您的推荐：')\n",
    "        p = np.squeeze(sim)\n",
    "        p[np.argsort(p)[:-top_k]] = 0\n",
    "        p = p / np.sum(p)\n",
    "        results = set()\n",
    "        while len(results) != 5:\n",
    "            c = np.random.choice(3883, 1, p=p)[0]\n",
    "            results.add(c)\n",
    "        for val in (results):\n",
    "            print(val)\n",
    "            print(movies_orig[val])\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n",
      "以下是给您的推荐：\n",
      "2469\n",
      "[2538 'Dancemaker (1998)' 'Documentary']\n",
      "1803\n",
      "[1872 'Go Now (1995)' 'Drama']\n",
      "911\n",
      "[923 'Citizen Kane (1941)' 'Drama']\n",
      "1180\n",
      "[1198 'Raiders of the Lost Ark (1981)' 'Action|Adventure']\n",
      "1950\n",
      "[2019\n",
      " 'Seven Samurai (The Magnificent Seven) (Shichinin no samurai) (1954)'\n",
      " 'Action|Drama']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{911, 1180, 1803, 1950, 2469}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend_your_favorite_movie(234, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 看过这个电影的人还看了（喜欢）哪些电影\n",
    "- 首先选出喜欢某个电影的top_k个人，得到这几个人的用户特征向量。\n",
    "- 然后计算这几个人对所有电影的评分\n",
    "- 选择每个人评分最高的电影作为推荐\n",
    "- 同样加入了随机选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def recommend_other_favorite_movie(movie_id_val, top_k=10):\n",
    "    loaded_graph = tf.Graph()\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load saved model\n",
    "        loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "        loader.restore(sess, load_dir)\n",
    "        \n",
    "        # 推荐您喜欢的电影\n",
    "        probs_movie_embeddings = (movie_matrics[movieid2idx[movie_id_val]]).reshape([1, 200])\n",
    "        probs_user_favorite_similarity = tf.matmul(probs_movie_embeddings, tf.transpose(user_matrics))\n",
    "        favorite_user_id = np.argsort(probs_user_favorite_similarity.eval())[0][-top_k:]\n",
    "        \n",
    "        print('您看的电影是：{}'.format(movies_orig[movieid2idx[movie_id_val]]))\n",
    "        print('喜欢看这个电影的人是：{}'.format(users_orig[favorite_user_id - 1]))\n",
    "        probs_users_embeddings = (user_matrics[favorite_user_id - 1]).reshape([-1, 200])\n",
    "        probs_similarity = tf.matmul(probs_users_embeddings, tf.transpose(movie_matrics))\n",
    "        sim = (probs_similarity.eval())\n",
    "                                     \n",
    "        p = np.argmax(sim, 1)\n",
    "        print('喜欢看这个电影的人还喜欢看')\n",
    "        results = set()\n",
    "        while len(results) != 5:\n",
    "            c = p[random.randrange(top_k)]\n",
    "            results.add(c)\n",
    "        for val in (results):\n",
    "            print(val)\n",
    "            print(movies_orig[val])\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n",
      "您看的电影是：[1401 'Ghosts of Mississippi (1996)' 'Drama']\n",
      "喜欢看这个电影的人是：[[5203 'F' 45 6]\n",
      " [4888 'F' 56 0]\n",
      " [4189 'M' 35 14]\n",
      " [155 'M' 35 12]\n",
      " [3763 'M' 25 2]\n",
      " [4884 'M' 35 14]\n",
      " [520 'F' 35 20]\n",
      " [2823 'M' 25 4]\n",
      " [5503 'F' 25 9]\n",
      " [2902 'M' 25 12]\n",
      " [4562 'M' 35 10]\n",
      " [3347 'F' 35 7]\n",
      " [2546 'F' 56 13]\n",
      " [1752 'M' 25 3]\n",
      " [4272 'M' 45 17]\n",
      " [52 'M' 18 4]\n",
      " [2737 'M' 25 3]\n",
      " [4903 'M' 35 12]\n",
      " [724 'M' 50 13]\n",
      " [3031 'M' 18 4]]\n",
      "喜欢看这个电影的人还喜欢看\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-020dd0a9364a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrecommend_other_favorite_movie\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1401\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-41-20e758961e99>\u001b[0m in \u001b[0;36mrecommend_other_favorite_movie\u001b[1;34m(movie_id_val, top_k)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m             \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mval\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "recommend_other_favorite_movie(1401, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结论\n",
    "以上就是实现的常用的推荐功能，将网络模型作为回归问题进行训练，得到训练好的用户特征矩阵和电影特征矩阵进行推荐。\n",
    "\n",
    "### 扩展阅读\n",
    "如果你对个性化推荐感兴趣，以下资料建议你看看：\n",
    "- [`Understanding Convolutional Neural Networks for NLP`](http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/)\n",
    "\n",
    "- [`Convolutional Neural Networks for Sentence Classification`](https://github.com/yoonkim/CNN_sentence)\n",
    "\n",
    "- [`利用TensorFlow实现卷积神经网络做文本分类`](http://www.jianshu.com/p/ed3eac3dcb39?from=singlemessage)\n",
    "\n",
    "- [`Convolutional Neural Network for Text Classification in Tensorflow`](https://github.com/dennybritz/cnn-text-classification-tf)\n",
    "\n",
    "- [`SVD Implement Recommendation systems`](https://github.com/songgc/TF-recomm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
