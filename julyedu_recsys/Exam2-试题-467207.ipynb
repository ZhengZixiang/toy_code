{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 七月在线《推荐系统实战》第二阶段考试\n",
    "\n",
    "\n",
    "- 起止时间：请同学在2018年12月19日至12月26日期间完成\n",
    "- 考试方式：请同学将该试卷进行**复制**后，并且**改名为自己的七月在线UID**后，如<font color=green>Exam2-253398.ipynb</font>，<b>移动</b>至<font color=green>/0.Teacher/Exam/2/</font>目录下后，进行答题。\n",
    "- 注意事项：为确保同学们真正了解自身对本周课程的掌握程度，<font color=red><b>请勿翻阅，移动，更改</b></font>其它同学试卷。如发现按0分处理\n",
    "- 请同学在下方同学姓名处填写自己的姓名和UID(学号)，批改人和最终得分不用填写\n",
    "- <font color=blue>试卷内容对应视频课程:第4课、第5课</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 同学姓名:<b> 纸箱 </b>  \n",
    "- 七月在线UID(学号):<b> 467207 </b>  \n",
    "- 批改人：\n",
    "- 最终得分: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简答题（共10题，每题5分，共50分）\n",
    "\n",
    "<font color=red size=5>题目编号仅用于判题方便,与题目顺序无关</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.简述你对点击率预估(CTR)的理解?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "点击率预估本质上是用户从impression到click的二分类概率问题，判断用户是否会点击推荐内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.当模型训练好的时候,遇到新的ID,怎么处理?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "遇到新的ID，可以先用热门的推荐内容去临时填充，日后再抽取新ID的历史信息让模型学习。或者引导user或item产生基本的画像信息，通过找embedding上的近似来做临时推荐。还可以通过找新ID所属的类目或者群体，取群体的平均值。\n",
    "\n",
    "没在第四课视频中仔细找到对应的内容，所以自己瞎写了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.推荐系统中的离线训练使用模型有哪些?(至少列出3种)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 低维线性模型 LR\n",
    "- Random Forest、CART、GBDT\n",
    "- FM、FFM\n",
    "- DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.特征交叉(特征组合)方式有哪些?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dense特征组合\n",
    "    1. 将一个特征与其本身或其他特征相乘（二阶或者高阶）\n",
    "    2. 两个特征相除\n",
    "    3. 对连续特征进行分桶，以分为多个区间分箱\n",
    "- ID特征之间的组合\n",
    "    - 笛卡尔积：假如拥有一个特征A，A有两个可能值{A1, A2}。拥有一个特征B，存在{B1, B2}等可能值。然后A&B之间的交叉特征如下：{(A1, B1), (A1, B2), (A2, B1), (A2, B2)}，比如经纬度，一个更好地诠释好的交叉特征的实例是类似于（经度，纬度）。一个相同的经度对应了地图上很多的地方，纬度也是一样。但是一旦将经度和纬度组合刀一起，它们就代表了地理上特定的一块区域，区域中每一部分是拥有着类似的特性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.特征选择的方法有哪些?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Filter: 过滤法，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。\n",
    "2. Wrapper: 包装法，根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。\n",
    "3. Embedded: 嵌入法，先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.简述AutoML流程?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Automated Feature Engineering\n",
    "2. Automated Architecture Search\n",
    "3. Automated Hyper-parameter Tuning\n",
    "4. Automated Model Selection\n",
    "5. Automated Model Ensemble\n",
    "6. Automated Model Distillation and Export for Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.多目标排序流程?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 召回（Match）得到候选\n",
    "2. 通过Parameter Server得到模型，通过数据库得到特征，Ranker得到不同目标（CTR、CVR、stay）的结果\n",
    "3. Reranker（Learning to rank）则根据2的多目标进行组合得到一个多目标排序分数\n",
    "4. 最后再由一个规则系统，如活动扶持、EE问题、新品扶持、低俗打压、流量控制等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.写出LTR评测系统-MAP公式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Precision at position n 前n个结果相关的数量\n",
    "$$P@n=\\frac{\\#\\{\\mathrm{relevant\\ documents\\ in\\ top}\\ n\\ \\mathrm{results}\\}}{n}$$\n",
    "\n",
    "- Average Precision 对每个n的相关数除以实际相关文档数\n",
    "$$AP=\\frac{\\sum_nP@1\\bullet I\\{\\mathrm{document}\\ n\\ \\mathrm{is relevant}\\}}{\\#\\mathrm{relavant \\ documents}}$$\n",
    "\n",
    "- MAP averaged over all queries in the test set 对所有请求的AP求平均"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.写出LTR evaluation-nDCG公式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$N(n)=Z_n\\sum^n_{j=1}(2^{r(j)-1})/\\log{(1+j)}$$\n",
    "\n",
    "- n: Normalization $Z_n$ 归一化因子\n",
    "- D: Position discount $/\\log{(1+j)}$ 位置越靠前，影响越大\n",
    "- C: Cumulating $\\sum^n_{j=1}$ 累加\n",
    "- G: Gain $2^{r(j)-1}$ 收益"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17.简述Multi-task learning(MLT)多任务学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多任务学习是迁移学习的一种，数据是相似的，但是任务不同。多任务学习的前提是多个任务具有相似性，可以共享底层特征，这样就可以通过对相似任务的模型fine-tuning从而迁移到新的任务。\n",
    "\n",
    "它有以下好处：\n",
    "1. 多任务学习可以解决数据系数的问题。\n",
    "2. 不同的任务的模型善于学习不同特征，使得模型的特征学习得更加充分。\n",
    "3. 由于引入归纳偏置（inductive bias），这样可以提高模型的泛化性，而不会过拟合某个单一目标。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 代码题（共1题可选，每题20分，共20分）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Wide & Deep Learning实战\n",
    "> **如使用老师给出的代码,需要学员对代码添加详细备注!  \n",
    "分数将会根据备注内容来评判学员对该demo的理解程度**\n",
    "\n",
    "<font color=blue>数据在群文件-->第四课 用户建模目录中-->Wide_Deep_Learning_demo.zip</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "老师给的代码是基于以前tf.contrib.layers下的API，版本更迭后，现在tf的API已经迁移到了\n",
    "f.feature_column, tf.estimator, tf.data这里基于上述API对原有的代码稍微修改了一下。\n",
    "\n",
    "此外由于数据文件有点问题，比如adult.test的label列多了一个`.`，分隔符`,空格`替换成`,`\n",
    "所以又实现了clean对数据稍微进行了清洗，转化成TextLineDataset API的读入数据方式，\n",
    "主要体现在input_fn函数的变动。\n",
    "\n",
    "网络结构上基本一致，代码重构为Estimator形式，acc为0.84左右。\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "import urllib\n",
    "\n",
    "COLUMNS = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'martial_status',\n",
    "           'occupation', 'relationship', 'race', 'gender', 'capital_gain', 'capital_loss',\n",
    "           'hours_per_week', 'native_country', 'income_bracket']\n",
    "LABEL_COLUMN = 'income_bracket'\n",
    "CATEGORICAL_COLUMNS = ['workclass', 'education', 'martial_status', 'occupation',\n",
    "                       'relationship', 'race', 'gender', 'native_country']\n",
    "CONTINUOUS_COLUMNS = ['age', 'education_num', 'capital_gain', 'capital_loss',\n",
    "                      'hours_per_week']\n",
    "COLUMN_DEFAULTS = [[0.0], [''], [0.0], [''], [0.0], [''], [''], [''], [''], [''], [0.0], [0.0], [0.0], [''], ['']]\n",
    "\n",
    "\n",
    "def maybe_download(train_data, test_data):\n",
    "    \"\"\"Maybe downloads training data and returns train and test file names.\"\"\"\n",
    "    if train_data:\n",
    "        train_file_name = train_data\n",
    "    else:\n",
    "        train_file = tempfile.NamedTemporaryFile(delete=False)\n",
    "        urllib.request.urlretrieve('http://mlr.cs.umass.edu/ml/machine-learning-databases/adult/adult.data', train_file.name)\n",
    "        train_file_name = train_file.name\n",
    "        train_file.close()\n",
    "        print('Training data is downloaded to %s' % train_file_name)\n",
    "\n",
    "    if test_data:\n",
    "        test_file_name = test_data\n",
    "    else:\n",
    "        test_file = tempfile.NamedTemporaryFile(delete=False)\n",
    "        urllib.request.urlretrieve('http://mlr.cs.umass.edu/ml/machine-learning-databases/adult/adult.test', test_file.name)\n",
    "        test_file_name = test_file.name\n",
    "        test_file.close()\n",
    "        print('Test data is downloaded to %s' % test_file_name)\n",
    "\n",
    "    return train_file_name, test_file_name\n",
    "\n",
    "def clean(train_file_name, test_file_name):\n",
    "    with open(train_file_name + '.clean', 'w') as wf:\n",
    "        with open(train_file_name, 'r') as rf:\n",
    "            for line in rf.readlines():\n",
    "                if line.strip() == '': continue\n",
    "                line = line.replace(', ', ',')\n",
    "                wf.write(line)\n",
    "    os.remove(train_file_name)\n",
    "    os.rename(train_file_name + '.clean', train_file_name)\n",
    "\n",
    "    with open(test_file_name + '.clean', 'w') as wf:\n",
    "        with open(test_file_name, 'r') as rf:\n",
    "            for i, line in enumerate(rf.readlines()):\n",
    "                if i == 0: continue\n",
    "                if line.strip() == '': continue\n",
    "                line = line.replace(', ', ',')\n",
    "                wf.write(line[:-2] + '\\n')\n",
    "    os.remove(test_file_name)\n",
    "    os.rename(test_file_name + '.clean', test_file_name)\n",
    "\n",
    "\n",
    "def build_model_columns():\n",
    "    # Sparse base columns.\n",
    "    workclass = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "        key='workclass', hash_bucket_size=100)\n",
    "    education = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "        key='education', hash_bucket_size=1000)\n",
    "    occupation = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "        key='occupation', hash_bucket_size=1000)\n",
    "    relationship = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "        key='relationship', hash_bucket_size=100)\n",
    "    gender = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "        key='gender', vocabulary_list=['female', 'male'])\n",
    "    native_country = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "        key='native_country', hash_bucket_size=1000)\n",
    "\n",
    "    # Continuous base columns.\n",
    "    age = tf.feature_column.numeric_column(key='age')\n",
    "    education_num = tf.feature_column.numeric_column(key='education_num')\n",
    "    capital_gain = tf.feature_column.numeric_column(key='capital_gain')\n",
    "    capital_loss = tf.feature_column.numeric_column(key='capital_loss')\n",
    "    hours_per_week = tf.feature_column.numeric_column(key='hours_per_week')\n",
    "\n",
    "    # Transformations.\n",
    "    bucketized_age = tf.feature_column.bucketized_column(\n",
    "        age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\n",
    "\n",
    "    # Crossed columns.\n",
    "    crossed_columns = [tf.feature_column.crossed_column(['education', 'occupation'], hash_bucket_size=int(1e4)),\n",
    "                       tf.feature_column.crossed_column([bucketized_age, 'education', 'occupation'], hash_bucket_size=int(1e6)),\n",
    "                       tf.feature_column.crossed_column(['native_country', 'occupation'], hash_bucket_size=int(1e4))]\n",
    "\n",
    "    # Wide and deep columns\n",
    "    base_columns = [workclass, education, occupation, relationship, gender, native_country, bucketized_age]\n",
    "    wide_columns = base_columns + crossed_columns\n",
    "    deep_columns = [tf.feature_column.embedding_column(workclass, dimension=8),\n",
    "                    tf.feature_column.embedding_column(education, dimension=8),\n",
    "                    tf.feature_column.embedding_column(relationship, dimension=8),\n",
    "                    tf.feature_column.embedding_column(gender, dimension=8),\n",
    "                    tf.feature_column.embedding_column(occupation, dimension=8),\n",
    "                    age, education_num, capital_gain, capital_loss, hours_per_week]\n",
    "\n",
    "    return wide_columns, deep_columns\n",
    "\n",
    "\n",
    "def build_estimator(model_dir, model_type):\n",
    "    \"\"\"Build an estimator appropriate for the given model type.\"\"\"\n",
    "    wide_columns, deep_columns = build_model_columns()\n",
    "    print(wide_columns)\n",
    "    print(deep_columns)\n",
    "    hidden_units = [256, 128, 64]\n",
    "\n",
    "    if model_type == 'wide':\n",
    "        return tf.estimator.LinearClassifier(model_dir=model_dir, feature_columns=wide_columns)\n",
    "    elif model_type == 'deep':\n",
    "        return tf.estimator.DNNClassifier(model_dir=model_dir, feature_columns=deep_columns,\n",
    "                                          hidden_units=hidden_units)\n",
    "    else:\n",
    "        return tf.estimator.DNNLinearCombinedClassifier(model_dir=model_dir,\n",
    "                                                        linear_feature_columns=wide_columns,\n",
    "                                                        dnn_feature_columns=deep_columns,\n",
    "                                                        dnn_hidden_units=hidden_units)\n",
    "\n",
    "\n",
    "def input_fn(data_file, shuffle, batch_size):\n",
    "    \"\"\"Input builder function.\"\"\"\n",
    "\n",
    "    def parse_data_file(value):\n",
    "        print('Parsing', data_file)\n",
    "        columns = tf.decode_csv(value, record_defaults=COLUMN_DEFAULTS, field_delim=',')\n",
    "        features = dict(zip(COLUMNS, columns))\n",
    "        labels = features.pop(LABEL_COLUMN)\n",
    "        return features, tf.equal(labels, '>50K')\n",
    "\n",
    "    # Extract lines from input files using the Dataset API.\n",
    "    dataset = tf.data.TextLineDataset(data_file)\n",
    "    dataset = dataset.map(parse_data_file)\n",
    "    if shuffle:\n",
    "        dataset.shuffle(buffer_size=10000)\n",
    "        dataset = dataset.repeat()\n",
    "\n",
    "    # We call repeat after shuffling, rather than before, to prevent seperate\n",
    "    # epochs from blending together.\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def main(_):\n",
    "    print(FLAGS)\n",
    "    train_file_name, test_file_name = maybe_download(FLAGS.train_data, FLAGS.test_data)\n",
    "    clean(train_file_name, test_file_name)\n",
    "\n",
    "    model_dir = tempfile.mkdtemp() if not FLAGS.model_dir else FLAGS.model_dir\n",
    "    print('model directory = %s' % model_dir)\n",
    "\n",
    "    model = build_estimator(FLAGS.model_dir, FLAGS.model_type)\n",
    "    train_spec = tf.estimator.TrainSpec(input_fn=lambda: input_fn(\n",
    "        train_file_name, True, FLAGS.batch_size), max_steps=FLAGS.train_steps)\n",
    "    eval_spec = tf.estimator.EvalSpec(input_fn=lambda: input_fn(\n",
    "        test_file_name, False, FLAGS.batch_size))\n",
    "    tf.estimator.train_and_evaluate(model, train_spec, eval_spec)\n",
    "\n",
    "    # Evaluate accuracy.\n",
    "    print('Evaluating...')\n",
    "    results = model.evaluate(input_fn=lambda: input_fn(\n",
    "        test_file_name, False, FLAGS.batch_size))\n",
    "    for key in sorted(results):\n",
    "        print('%s: %s' % (key, results[key]))\n",
    "    print('Train WDL End.')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.register('type', 'bool', lambda v: v.lower() == 'true')\n",
    "    parser.add_argument('--model_dir', type=str, default='./wdl_data/model_save',\n",
    "                        help='Base directory for output models.')\n",
    "    parser.add_argument('--model_type', type=str, default='wide_deep',\n",
    "                        help=\"Valid model types:{'wide', 'deep', 'wide_deep'}.\")\n",
    "    parser.add_argument('--train_steps', type=int, default=2000,\n",
    "                        help='Number of training steps.')\n",
    "    parser.add_argument('--batch_size', type=int, default=128,\n",
    "                        help='Number of examples per batch')\n",
    "    parser.add_argument('--train_data', type=str, default='./wdl_data/adult.data',\n",
    "                        help='Path to the training data.')\n",
    "    parser.add_argument('--test_data', type=str, default='./wdl_data/adult.test',\n",
    "                        help='Path to the test data.')\n",
    "    FLAGS, unparsed = parser.parse_known_args()\n",
    "    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 思考题(共3题,每题10分,共30分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 样本和特征如何构造,用点击数据、还是浏览数据?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我认为这是个目标驱动的问题，如果推荐系统更注重CTR，那自然是选择点击数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. CTR和CVR强相关,是否要构造两个模型?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以构造两个模型，来分别学习CTR和CVR两个任务。\n",
    "\n",
    "但是由于强相关，而CVR的样本数据其实是CTR的一个子样本空间，只通过用户impression到click的数据来学习CVR很可能是有偏的，而且模型泛化能力也是个问题。因此多任务学习可以在此处应用，这正是ESMM这个模型的motivation。以ESMM为例，从它的模型架构来看，是通过CTR和CTCVR两个目标来间接的学习到真实样本空间的CVR。既可以认为是两个模型的连接，但是由于ESMM模型底层embedding的共享，也可以认为本质上是一个模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 如何应对大促、季节等用户行为的变化对模型的影响 ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在构造模型时加入用户的时间序列行为信息，来应对大促、季节等时间因素的影响。此外，实时学习来保证模型的最新。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 附加题(不计入分数)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.使用公开数据集实现ESMM\n",
    "<font color=red>数据集在GPU服务器的 `/New_User/data_store/sample_train\n",
    "`目录中,因数据集过大,同学们使用过程最好在晚上空闲时段进行.</font>\n",
    "\n",
    "<font color=red>代码粘贴到本试题下方即可,代码需要有详细注释.<b>不允许在作业考试平台跑任何大型项目</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
