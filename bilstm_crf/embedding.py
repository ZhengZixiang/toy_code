# -*- coding: utf-8 -*-
import torch
import torch.nn as nn
import torch.nn.functional as F

from parameters import *


class Embedding(nn.Module):
    def __init__(self, char_vocab_size, word_vocab_size, embed_size):
        super(Embedding, self).__init__()
        dim = embed_size // len(EMBED)  # dimension of each embedding vector

        # architecture
        if 'char' in EMBED:
            self.char_embed = EmbedCNN(char_vocab_size, dim)
        if 'word' in EMBED:
            self.word_embed = nn.Embedding(word_vocab_size, dim, padding_idx=PAD_IDX)

    def forward(self, xc, xw):
        hc = self.char_embed(xc) if 'char' in EMBED else []
        hw = self.word_embed(xw) if 'word' in EMBED else []
        h = torch.cat([hc, hw], 2)
        return h


class EmbedCNN(nn.Module):
    def __init__(self, dim_in, dim_out):
        super(EmbedCNN, self).__init__()
        self.embed_size = 50
        self.num_feature_maps = 50  # feature maps generated by each kernel
        self.kernel_sizes = [3]

        # architecture
        self.embed = nn.Embedding(dim_in, self.embed_size, padding_idx=PAD_IDX)
        self.convs = nn.ModuleList([nn.Conv2d(
            in_channels=1,
            out_channels=self.num_feature_maps,
            kernel_size=(i, self.embed_size)
        ) for i in self.kernel_sizes])
        self.dropout = nn.Dropout(DROPOUT)
        self.fc = nn.Linear(len(self.kernel_sizes) * self.num_feature_maps, dim_out)

    def forward(self, x):
        x = x.view(-1, x.size(2))  # [batch_size (B) * word_seq_len (L), char_seq_len (H)]
        x = self.embed(x)  # [B * L, H, embed_size (W)]
        x = x.unsqueeze(1)  # [B * L, Ci, H, W]
        h = [conv(x) for conv in self.convs]  # [B * L, Co, H, 1] * len(kernel_size) (K)
        h = [F.relu(k).squeeze(3) for k in h]  # [B * L, Co, H] * K
        h = [F.max_pool1d(k, k.size(2)) for k in h]  # [B * L, Co] * K
        h = torch.cat(h, 1)  # [B * L, Co * K]
        h = self.dropout(h)
        h = self.fc(h)  # [B * L, dim_out]
        h = h.view(BATCH_SIZE, -1, h.size(1))  # [B, L, dim_out]
        return h
